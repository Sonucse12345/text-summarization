# script parameters
model_id: "meta-llama/llama-70b-h"     # Hugging Face model id
dataset_path: "./"                     # path to dataset
max_seq_len: 3072                      # max sequence length for model and packing of the dataset
# training parameters
output_dir: "./llama-70b-hf-no-robot"  # Temporary output directory for model checkpoints
report_to: "tensorboard"              # report metrics to tensorboard
learning_rate: 2e-4                   # learning rate
lr_scheduler_type: "constant"             # learning rate scheduler
num_train_epochs: 3                   # number of training epochs
per_device_train_batch_size: 1        # batch size per device during training
gradient_accumulation_steps: 4        # number of steps before performing a backward/update pass
optim: adamw_torch_fused              # use fused adamw optimizer
logging_steps: 10                     # log every 10 steps
save_strategy: epoch                  # save checkpoint every epoch
max_grad_norm: 0.3                    # max gradient norm
warmup_ratio: 0.03                    # warmup ratio
bf16: true                            # use bfloat16 precision
tf32: true                            # use tf32 precision
gradient_checkpointing: true          # use gradient checkpointing to save memory
# FSDP parameters: https://huggingface.co/docs/transformers/main/en/fsdp
fsdp_config:
  fsdp_auto_wrap_policy: TRANSFORMER_BASED_WRAP # auto wrap policy
  fsdp_backward_prefetch: BACKWARD_PRE # backward prefetch
  fsdp_cpu_ram_efficient_loading: true # cpu ram efficient loading
  fsdp_forward_prefetch: false # forward prefetch
  fsdp_offload_params: false # offload params
  fsdp_sharding_strategy: FULL_SHARD # sharding strategy
  fsdp_state_dict_type: SHARDED_STATE_DICT # state dict type
  fsdp_sync_module_states: true # sync module states
  fsdp_use_orig_params: false # use original params
