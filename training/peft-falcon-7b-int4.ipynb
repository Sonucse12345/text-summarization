{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Instruction-Tune Falcon 7B using PEFT and QLoRA with int-4 \n",
    "\n",
    "In this blog, we are going to learn how to to fine-tune [Falcon-7B](https://huggingface.co/tiiuae/falcon-7b) using [PEFT](https://github.com/huggingface/peft) with [Low-Rank Adaptation of Large Language Models (LoRA)](https://arxiv.org/abs/2106.09685). We are going to instruct-fine-tune Falcon using the new [SFTTrainer](https://huggingface.co/docs/trl/main/en/sft_trainer) from the [trl](https://github.com/lvwerra/trl) library\n",
    "\n",
    "We will learn how to:\n",
    "1. Setup Development Environment and prepare the dataset\n",
    "2. Fine-Tune Falcon-7B with QLoRA in int-4\n",
    "3. Test Model and run Inference\n",
    "\n",
    "### Quick intro: PEFT or Parameter Efficient Fine-tuning\n",
    "\n",
    "[PEFT](https://github.com/huggingface/peft), or Parameter Efficient Fine-tuning, is a new open-source library from Hugging Face to enable efficient adaptation of LLMs to various downstream applications without fine-tuning all the model's parameters. PEFT currently includes techniques for:\n",
    "\n",
    "- LoRA:Â [LORA: LOW-RANK ADAPTATION OF LARGE LANGUAGE MODELS](https://arxiv.org/pdf/2106.09685.pdf)\n",
    "- Prefix Tuning:Â [P-Tuning v2: Prompt Tuning Can Be Comparable to Fine-tuning Universally Across Scales and Tasks](https://arxiv.org/pdf/2110.07602.pdf)\n",
    "- P-Tuning:Â [GPT Understands, Too](https://arxiv.org/pdf/2103.10385.pdf)\n",
    "- Prompt Tuning:Â [The Power of Scale for Parameter-Efficient Prompt Tuning](https://arxiv.org/pdf/2104.08691.pdf)\n",
    "\n",
    "*Note: This tutorial was created and run on a g5.48xlarge AWS EC2 Instance, including 1 NVIDIA A10G.*"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup Development Environment and prepare the dataset\n",
    "\n",
    "In our example, we use the [PyTorch Deep Learning AMI](https://docs.aws.amazon.com/dlami/latest/devguide/tutorial-pytorch.html) with already set up CUDA drivers and PyTorch installed. We still have to install the Hugging Face Libraries, including transformers and datasets. Running the following cell will install all the required packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install \"git+https://github.com/huggingface/peft.git@189a6b8e357ecda05ccde13999e4c35759596a67\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# install Hugging Face Libraries\n",
    "# !pip install \"peft==0.3.0\" \"trl==0.4.4\" \"transformers==4.30.1\" \"datasets==2.12.0\" \"accelerate==0.20.3\" \"evaluate==0.4.0\" \"torch==2.0.1\" \"bitsandbytes==0.39.0\" --upgrade --quiet\n",
    "!pip install  \"trl==0.4.4\" \"transformers==4.30.1\" \"datasets==2.12.0\" \"accelerate==0.20.3\" \"evaluate==0.4.0\" \"torch==2.0.1\" \"bitsandbytes==0.39.0\" --upgrade --quiet\n",
    "!pip install \"git+https://github.com/huggingface/peft.git@189a6b8e357ecda05ccde13999e4c35759596a67\"\n",
    "# install additional dependencies needed for training\n",
    "!pip install tensorboard einops loralib"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we will use theÂ [dolly](https://huggingface.co/datasets/databricks/databricks-dolly-15k)Â an open source dataset of instruction-following records generated by thousands of Databricks employees in several of the behavioral categories outlined in the [InstructGPT paper](https://arxiv.org/abs/2203.02155), including brainstorming, classification, closed QA, generation, information extraction, open QA, and summarization.\n",
    "\n",
    "```python\n",
    "{\n",
    "  \"instruction\": \"What is world of warcraft\",\n",
    "  \"context\": \"\",\n",
    "  \"response\": \"World of warcraft is a massive online multi player role playing game. It was released in 2004 by bizarre entertainment\"\n",
    "}\n",
    "```\n",
    "\n",
    "> Note: The next steps are for demonstration. The dataset processing, formatting and tokenization will be part of the training script, [run_clm_fsdp_lora.py](./scripts/run_clm_fsdp_lora.py). \n",
    "\n",
    "To load theÂ `databricks/databricks-dolly-15k`Â dataset, we use theÂ `load_dataset()`Â method from the ðŸ¤— Datasets library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from random import randrange\n",
    "# Load dataset from the hub\n",
    "dataset = load_dataset(\"databricks/databricks-dolly-15k\", split=\"train\")\n",
    "\n",
    "print(f\"dataset size: {len(dataset)}\")\n",
    "print(dataset[randrange(len(dataset))])\n",
    "# Train dataset size: 14732\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To instruct tune our model we need to convert our structured examples into a collection of tasks described via instructions. Here is where the `SFTTrainer` from `trl` comes handy. The `SFTTrainer` supports formatting during training. This means we only need to define a `formatting_function` that takes a sample and returns a string with our format instruction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_dolly(sample):\n",
    "  instruction = f\"### Instruction\\n{sample['instruction']}\"\n",
    "  context = f\"### Context\\n{sample['context']}\" if len(sample['context']) > 0 else None\n",
    "  response = f\"### Answer\\n{sample['response']}\"\n",
    "  # join all the parts together\n",
    "  prompt = \"\\n\\n\".join([i for i in [instruction, context, response] if i is not None])\n",
    "  return prompt"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "lets test our formatting function on a random example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Instruction\n",
      "What are the ways to save money in gardening?\n",
      "\n",
      "### Answer\n",
      "1. Avoid buying potting mix by making your own potting soil\n",
      "2. Compost your food scraps to make your own soil\n",
      "3. Avoid buying seed germinating trays by using tofu trays and other recycled food trays to germinate seeds\n",
      "4. Avoid buying pots and containers by re-using plastic milk containers with the top cut off, tetra pak with the top cut off, yoghurt containers, plastic soda bottles etc.\n",
      "5. Avoid buying plants from the store by germinating plants from seed yourself\n",
      "6. Collect rainwater for your plants to avoid using municipal water\n",
      "7. Re-use water from rinsing vegetables/rice to water your plants to minimize the use of municipal water\n"
     ]
    }
   ],
   "source": [
    "from random import randrange\n",
    "\n",
    "print(format_dolly(dataset[randrange(len(dataset))]))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Fine-Tune Falcon-7B with QLoRA in int-4\n",
    "\n",
    "We are going to use the recently introduced method in the paper \"[QLoRA: Quantization-aware Low-Rank Adapter Tuning for Language Generation](https://arxiv.org/abs/2106.09685)\" by Tim Dettmers et al. QLoRA is a new technique to reduce the memory footprint of large language models during finetuning, without sacrificing performance. The TL;DR; of how QLoRA works is: \n",
    "\n",
    "* Quantize the pretrained model to 4 bits and freezing it.\n",
    "* Attach small, trainable adapter layers. (LoRA)\n",
    "* Finetune only the adapter layers, while using the frozen quantized model for context.\n",
    "\n",
    "If you want to learn more about QLoRA and how it works I recommend you to read the [Making LLMs even more accessible with bitsandbytes, 4-bit quantization and QLoRA](https://huggingface.co/blog/4bit-transformers-bitsandbytes) blog post.\n",
    "\n",
    "\n",
    "First, we are going to load our model together with our quantization configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "\n",
    "# Hugging Face model id\n",
    "model_id = \"ybelkada/falcon-7b-sharded-bf16\" # sharded weights\n",
    "\n",
    "# BitsAndBytesConfig int-4 config \n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "# Load model and tokenizer\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id, quantization_config=bnb_config, device_map={\"\":0},    trust_remote_code=True)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id, trust_remote_code=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `SFTTrainer` also supports a native integration with `peft`, which makes it super easy to efficiently instruction tune LLMs. We prepared a [run_clm_fsdp_lora.py](./scripts/run_clm_fsdp_lora.py), which implements causal language modeling and accepts all relevant parameters, including the model id, peft configuration. The `SFTTrainer` part in our scripts looks like this:\n",
    "\n",
    "```python\n",
    "trainer = SFTTrainer(\n",
    "    model, # our loaded model\n",
    "    args=training_args, # our training args\n",
    "    train_dataset=dataset, # raw training dataset\n",
    "    formatting_func=format_dolly, # formatting function\n",
    "    peft_config=peft_config, # peft config\n",
    "    packing=True, # wether to pack data samples to max length\n",
    "    max_seq_length=2048 # max sequence length for packing\n",
    ")\n",
    "```\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://colab.research.google.com/drive/1BiQiw31DT7-cDp1-0ySXvvhzqomTdI-o?usp=sharing#scrollTo=dQdvjTYTT1vQ\n",
    "https://gist.github.com/pacman100/1731b41f7a90a87b457e8c5415ff1c14"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%bash\n",
    "!python scripts/run_clm_fsdp_lora.py \\\n",
    " --model_id tiiuae/falcon-7b \\\n",
    " --dataset_id \"databricks/databricks-dolly-15k\" \\\n",
    " --per_device_train_batch_size 1 \\\n",
    " --num_train_epochs 1 \\\n",
    " --learning_rate 2e-4 \\\n",
    " --gradient_checkpointing True \\\n",
    " --bf16 True \\\n",
    " --tf32 True \\\n",
    " --output_dir ./tmp \\\n",
    " --logging_steps 10\n",
    " \n",
    " \n",
    " #--optim adamw_apex_fused \\"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "MODEL_ID=\"tiiuae/falcon-40b\"\n",
    "DATASET_ID=\"databricks/databricks-dolly-15k\"\n",
    "NUM_GPUS=8\n",
    "\n",
    "echo \"Training ${MODEL_ID} on ${DATASET_ID} using ${NUM_GPUS} GPU.\n",
    "\n",
    "torchrun --nproc_per_node ${NUM_GPUS} scripts/run_clm_fsdp_lora.py \\\n",
    "  --model_id ${MODEL_ID} \\\n",
    "  --dataset_id ${DATASET_ID} \\\n",
    "  --per_device_train_batch_size 1 \\\n",
    "  --num_train_epochs 1 \\\n",
    "  --learning_rate 2e-4 \\\n",
    "  --gradient_checkpointing True \\\n",
    "  --bf16 True \\\n",
    "  --tf32 True \\\n",
    "  --output_dir ./tmp \\\n",
    "  --logging_steps 10 \\\n",
    "  --fsdp \"full_shard auto_wrap\" \\\n",
    "  --fsdp_transformer_layer_cls_to_wrap \"DecoderLayer\"\n",
    "  # --optim adamw_apex_fused \\"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training took ~10:36:00 and cost `~13.22$` for 10h of training. For comparison a [full fine-tuning on FLAN-T5-XXL](https://www.philschmid.de/fine-tune-flan-t5-deepspeed#3-results--experiments) with the same duration (10h) requires 8x A100 40GBs and costs ~322$. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Test Model and run Inference\n",
    "\n",
    "After the training is done we want to run and test our model. We will use `peft` and `transformers` to load our LoRA adapter into our model. We will also use `accelerate` to run our inference on multiple GPUs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from peft import PeftModel, PeftConfig\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "# Load peft config for pre-trained checkpoint etc. \n",
    "peft_model_id = \"tmp\"\n",
    "config = PeftConfig.from_pretrained(peft_model_id)\n",
    "\n",
    "# load base LLM model and tokenizer\n",
    "model = AutoModelForCausalLM.from_pretrained(config.base_model_name_or_path,device_map=\"auto\", torch_dtype=torch.bfloat16)\n",
    "tokenizer = AutoTokenizer.from_pretrained(config.base_model_name_or_path)\n",
    "\n",
    "# Load the Lora model\n",
    "model = PeftModel.from_pretrained(model, peft_model_id, device_map={\"\":0})\n",
    "model.eval()\n",
    "\n",
    "print(\"Peft model loaded\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Letâ€™s load the dataset again with a random sample to try the summarization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset \n",
    "from random import randrange\n",
    "\n",
    "\n",
    "# Load dataset from the hub and get a sample\n",
    "dataset = load_dataset(\"databricks/databricks-dolly-15k\", split=\"train\")\n",
    "sample = dataset[randrange(len(dataset))]\n",
    "\n",
    "prompt = f\"### Instruction\\n{sample['instruction']}\\n\\n\"\n",
    "if len(sample['context']) > 0:\n",
    "  prompt += f\"### Context\\n{sample['context']}\\n\\n\"\n",
    "prompt += f\"### Answer\\n\"\n",
    "\n",
    "input_ids = tokenizer(prompt, return_tensors=\"pt\", truncation=True).input_ids.cuda()\n",
    "# with torch.inference_mode():\n",
    "outputs = model.generate(input_ids=input_ids, max_new_tokens=50, do_sample=True, top_p=0.9)\n",
    "\n",
    "print(f\"{tokenizer.batch_decode(outputs.detach().cpu().numpy(), skip_special_tokens=True)[0]}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nice! our model works! If want to accelerate our model we can deploy it with [Text Generation Inference](https://github.com/huggingface/text-generation-inference). Therefore we would need to merge our adapter weights into the base model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge LoRA and base model\n",
    "merged_model = model.merge_and_unload()\n",
    "\n",
    "# Save the merged model\n",
    "merged_model.save_pretrained(\"merged_model\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "2d58e898dde0263bc564c6968b04150abacfd33eed9b19aaa8e45c040360e146"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
