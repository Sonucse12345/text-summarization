{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to Fine-Tune LLMs in 2024 with TRL\n",
    "\n",
    "Large Language Models or LLMs have seen a lot of progress in the last year. We went from now ChatGPT competitor to a whole zoo of LLMs, including Meta AI's [Llama 2](https://huggingface.co/meta-llama/Llama-2-70b-chat-hf), Mistrals [Mistral](https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.2) & [Mixtral](https://huggingface.co/mistralai/Mixtral-8x7B-Instruct-v0.1) models, TII [Falcon](https://huggingface.co/tiiuae/falcon-40b), and many more. \n",
    "Those LLMs can be used for a variety of tasks, including chatbots, question answering, summarization without any additional training. However, if you want to customize a model for your application. You may need to fine-tune the model on your data to achieve higher quality results than prompting or saving cost by training smaller models more efficient model.\n",
    "\n",
    " \n",
    "This blog post walks you thorugh how to fine-tune open LLMs using Hugging Face [TRL](https://huggingface.co/docs/trl/index), [Transformers](https://huggingface.co/docs/transformers/index) & [datasets](https://huggingface.co/docs/datasets/index) in 2024. In the blog, we are going to:\n",
    "\n",
    "1. Define our use case \n",
    "2. Setup development environment\n",
    "3. Create and prepare the dataset\n",
    "4. Fine-tune LLM using `trl` and the `SFTTrainer` \n",
    "5. Test and evaluate the LLM\n",
    "6. Deploy the LLM for Production\n",
    "\n",
    "_Note: This blog was created to run on consumer size GPUs (24GB), e.g. NVIDIA A10G or RTX 4090/3090, but can be easily adapted to run on bigger GPUs._\n",
    "\n",
    "\n",
    "## 1. Define our use case \n",
    "\n",
    "When fine-tuning LLMs, it is important you know your use case and the task you want to solve. This will help you to choose the right model or help you to create a dataset to fine-tune your model. If you haven't defined your use case yet. You might want to go back to the drawing board. \n",
    "I want to mention that not all use cases require fine-tuning and it is always recommended to evaluate and try out already fine-tuned models or API-based models before fine-tuning your own model. \n",
    "\n",
    "As an example, we are going to use the following use case:\n",
    "\n",
    "> We want to fine-tune a model, which can generate SQL queries based on a natural language instruction, which can then be integrated into our BI tool. The goal is to reduce the time it takes to create a SQL query and make it easier for non-technical users to create SQL queries. \n",
    "\n",
    "Text to SQL can be a good use case for fine-tuning LLMs, as it is a complex task that requires a lot of (internal) knowledge about the data and the SQL language. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Setup development environment\n",
    "\n",
    "Our first step is to install Hugging Face Libraries and Pyroch, including trl, transformers and datasets. If you haven't heard of trl yet, don't worry. It is a new library on top of transformers and datasets, which makes it easier to fine-tune, rlhf, align open LLMs. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing installation: trl 0.7.10.dev0\n",
      "Uninstalling trl-0.7.10.dev0:\n",
      "  Successfully uninstalled trl-0.7.10.dev0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+https://github.com/philschmid/trl@fix-template\n",
      "  Cloning https://github.com/philschmid/trl (to revision fix-template) to /tmp/pip-req-build-29m6r4tx\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/philschmid/trl /tmp/pip-req-build-29m6r4tx\n",
      "  Running command git checkout -b fix-template --track origin/fix-template\n",
      "  Switched to a new branch 'fix-template'\n",
      "  Branch 'fix-template' set up to track remote branch 'fix-template' from 'origin'.\n",
      "  Resolved https://github.com/philschmid/trl to commit 8f60b2b40a5da916fced83cfba4fd516df28fc30\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: torch>=1.4.0 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from trl==0.7.10.dev0) (2.1.2)\n",
      "Requirement already satisfied: transformers>=4.31.0 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from trl==0.7.10.dev0) (4.36.2)\n",
      "Requirement already satisfied: numpy>=1.18.2 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from trl==0.7.10.dev0) (1.26.2)\n",
      "Requirement already satisfied: accelerate in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from trl==0.7.10.dev0) (0.26.1)\n",
      "Requirement already satisfied: datasets in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from trl==0.7.10.dev0) (2.16.1)\n",
      "Requirement already satisfied: tyro>=0.5.11 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from trl==0.7.10.dev0) (0.6.3)\n",
      "Requirement already satisfied: filelock in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from torch>=1.4.0->trl==0.7.10.dev0) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from torch>=1.4.0->trl==0.7.10.dev0) (4.8.0)\n",
      "Requirement already satisfied: sympy in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from torch>=1.4.0->trl==0.7.10.dev0) (1.12)\n",
      "Requirement already satisfied: networkx in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from torch>=1.4.0->trl==0.7.10.dev0) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from torch>=1.4.0->trl==0.7.10.dev0) (3.1.2)\n",
      "Requirement already satisfied: fsspec in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from torch>=1.4.0->trl==0.7.10.dev0) (2023.10.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from torch>=1.4.0->trl==0.7.10.dev0) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from torch>=1.4.0->trl==0.7.10.dev0) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from torch>=1.4.0->trl==0.7.10.dev0) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from torch>=1.4.0->trl==0.7.10.dev0) (8.9.2.26)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from torch>=1.4.0->trl==0.7.10.dev0) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from torch>=1.4.0->trl==0.7.10.dev0) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from torch>=1.4.0->trl==0.7.10.dev0) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from torch>=1.4.0->trl==0.7.10.dev0) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from torch>=1.4.0->trl==0.7.10.dev0) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.18.1 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from torch>=1.4.0->trl==0.7.10.dev0) (2.18.1)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from torch>=1.4.0->trl==0.7.10.dev0) (12.1.105)\n",
      "Requirement already satisfied: triton==2.1.0 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from torch>=1.4.0->trl==0.7.10.dev0) (2.1.0)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.4.0->trl==0.7.10.dev0) (12.3.101)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from transformers>=4.31.0->trl==0.7.10.dev0) (0.20.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from transformers>=4.31.0->trl==0.7.10.dev0) (21.3)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from transformers>=4.31.0->trl==0.7.10.dev0) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from transformers>=4.31.0->trl==0.7.10.dev0) (2023.12.25)\n",
      "Requirement already satisfied: requests in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from transformers>=4.31.0->trl==0.7.10.dev0) (2.31.0)\n",
      "Requirement already satisfied: tokenizers<0.19,>=0.14 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from transformers>=4.31.0->trl==0.7.10.dev0) (0.15.0)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from transformers>=4.31.0->trl==0.7.10.dev0) (0.4.1)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from transformers>=4.31.0->trl==0.7.10.dev0) (4.66.1)\n",
      "Requirement already satisfied: docstring-parser>=0.14.1 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from tyro>=0.5.11->trl==0.7.10.dev0) (0.15)\n",
      "Requirement already satisfied: rich>=11.1.0 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from tyro>=0.5.11->trl==0.7.10.dev0) (13.7.0)\n",
      "Requirement already satisfied: shtab>=1.5.6 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from tyro>=0.5.11->trl==0.7.10.dev0) (1.6.5)\n",
      "Requirement already satisfied: psutil in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from accelerate->trl==0.7.10.dev0) (5.9.5)\n",
      "Requirement already satisfied: pyarrow>=8.0.0 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from datasets->trl==0.7.10.dev0) (14.0.1)\n",
      "Requirement already satisfied: pyarrow-hotfix in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from datasets->trl==0.7.10.dev0) (0.6)\n",
      "Requirement already satisfied: dill<0.3.8,>=0.3.0 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from datasets->trl==0.7.10.dev0) (0.3.7)\n",
      "Requirement already satisfied: pandas in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from datasets->trl==0.7.10.dev0) (2.1.3)\n",
      "Requirement already satisfied: xxhash in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from datasets->trl==0.7.10.dev0) (3.4.1)\n",
      "Requirement already satisfied: multiprocess in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from datasets->trl==0.7.10.dev0) (0.70.15)\n",
      "Requirement already satisfied: aiohttp in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from datasets->trl==0.7.10.dev0) (3.9.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from aiohttp->datasets->trl==0.7.10.dev0) (23.1.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from aiohttp->datasets->trl==0.7.10.dev0) (6.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from aiohttp->datasets->trl==0.7.10.dev0) (1.9.4)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from aiohttp->datasets->trl==0.7.10.dev0) (1.4.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from aiohttp->datasets->trl==0.7.10.dev0) (1.3.1)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from aiohttp->datasets->trl==0.7.10.dev0) (4.0.3)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from packaging>=20.0->transformers>=4.31.0->trl==0.7.10.dev0) (3.1.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from requests->transformers>=4.31.0->trl==0.7.10.dev0) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from requests->transformers>=4.31.0->trl==0.7.10.dev0) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from requests->transformers>=4.31.0->trl==0.7.10.dev0) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from requests->transformers>=4.31.0->trl==0.7.10.dev0) (2023.11.17)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from rich>=11.1.0->tyro>=0.5.11->trl==0.7.10.dev0) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from rich>=11.1.0->tyro>=0.5.11->trl==0.7.10.dev0) (2.17.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from jinja2->torch>=1.4.0->trl==0.7.10.dev0) (2.1.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from pandas->datasets->trl==0.7.10.dev0) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from pandas->datasets->trl==0.7.10.dev0) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from pandas->datasets->trl==0.7.10.dev0) (2023.3)\n",
      "Requirement already satisfied: mpmath>=0.19 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from sympy->torch>=1.4.0->trl==0.7.10.dev0) (1.3.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich>=11.1.0->tyro>=0.5.11->trl==0.7.10.dev0) (0.1.2)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets->trl==0.7.10.dev0) (1.16.0)\n",
      "Building wheels for collected packages: trl\n",
      "  Building wheel for trl (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for trl: filename=trl-0.7.10.dev0-py3-none-any.whl size=150973 sha256=01551c71fc0f2769c17fe5a68cd3df30e3793289a8e65f4937f6991122bf98d5\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-7kl8_bls/wheels/fc/8d/e8/55fd08ba7494a9850e8f81310b309c320e4d38bf665ae28db0\n",
      "Successfully built trl\n",
      "Installing collected packages: trl\n",
      "Successfully installed trl-0.7.10.dev0\n"
     ]
    }
   ],
   "source": [
    "!pip uninstall -y trl\n",
    "!pip install git+https://github.com/philschmid/trl@fix-template --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+https://github.com/huggingface/peft@62237dc9b1e460825462d6b1e7b0cf3418cec357\n",
      "  Cloning https://github.com/huggingface/peft (to revision 62237dc9b1e460825462d6b1e7b0cf3418cec357) to /tmp/pip-req-build-pqaqj9rh\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/huggingface/peft /tmp/pip-req-build-pqaqj9rh\n",
      "  Running command git rev-parse -q --verify 'sha^62237dc9b1e460825462d6b1e7b0cf3418cec357'\n",
      "  Running command git fetch -q https://github.com/huggingface/peft 62237dc9b1e460825462d6b1e7b0cf3418cec357\n",
      "  Resolved https://github.com/huggingface/peft to commit 62237dc9b1e460825462d6b1e7b0cf3418cec357\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from peft==0.7.2.dev0) (1.26.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from peft==0.7.2.dev0) (21.3)\n",
      "Requirement already satisfied: psutil in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from peft==0.7.2.dev0) (5.9.5)\n",
      "Requirement already satisfied: pyyaml in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from peft==0.7.2.dev0) (6.0.1)\n",
      "Requirement already satisfied: torch>=1.13.0 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from peft==0.7.2.dev0) (2.1.2)\n",
      "Requirement already satisfied: transformers in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from peft==0.7.2.dev0) (4.36.2)\n",
      "Requirement already satisfied: tqdm in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from peft==0.7.2.dev0) (4.66.1)\n",
      "Requirement already satisfied: accelerate>=0.21.0 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from peft==0.7.2.dev0) (0.26.1)\n",
      "Requirement already satisfied: safetensors in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from peft==0.7.2.dev0) (0.4.1)\n",
      "Requirement already satisfied: huggingface-hub>=0.17.0 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from peft==0.7.2.dev0) (0.20.2)\n",
      "Requirement already satisfied: filelock in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from huggingface-hub>=0.17.0->peft==0.7.2.dev0) (3.13.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from huggingface-hub>=0.17.0->peft==0.7.2.dev0) (2023.10.0)\n",
      "Requirement already satisfied: requests in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from huggingface-hub>=0.17.0->peft==0.7.2.dev0) (2.31.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from huggingface-hub>=0.17.0->peft==0.7.2.dev0) (4.8.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from packaging>=20.0->peft==0.7.2.dev0) (3.1.1)\n",
      "Requirement already satisfied: sympy in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from torch>=1.13.0->peft==0.7.2.dev0) (1.12)\n",
      "Requirement already satisfied: networkx in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from torch>=1.13.0->peft==0.7.2.dev0) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from torch>=1.13.0->peft==0.7.2.dev0) (3.1.2)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from torch>=1.13.0->peft==0.7.2.dev0) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from torch>=1.13.0->peft==0.7.2.dev0) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from torch>=1.13.0->peft==0.7.2.dev0) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from torch>=1.13.0->peft==0.7.2.dev0) (8.9.2.26)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from torch>=1.13.0->peft==0.7.2.dev0) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from torch>=1.13.0->peft==0.7.2.dev0) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from torch>=1.13.0->peft==0.7.2.dev0) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from torch>=1.13.0->peft==0.7.2.dev0) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from torch>=1.13.0->peft==0.7.2.dev0) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.18.1 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from torch>=1.13.0->peft==0.7.2.dev0) (2.18.1)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from torch>=1.13.0->peft==0.7.2.dev0) (12.1.105)\n",
      "Requirement already satisfied: triton==2.1.0 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from torch>=1.13.0->peft==0.7.2.dev0) (2.1.0)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.13.0->peft==0.7.2.dev0) (12.3.101)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from transformers->peft==0.7.2.dev0) (2023.12.25)\n",
      "Requirement already satisfied: tokenizers<0.19,>=0.14 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from transformers->peft==0.7.2.dev0) (0.15.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from jinja2->torch>=1.13.0->peft==0.7.2.dev0) (2.1.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from requests->huggingface-hub>=0.17.0->peft==0.7.2.dev0) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from requests->huggingface-hub>=0.17.0->peft==0.7.2.dev0) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from requests->huggingface-hub>=0.17.0->peft==0.7.2.dev0) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from requests->huggingface-hub>=0.17.0->peft==0.7.2.dev0) (2023.11.17)\n",
      "Requirement already satisfied: mpmath>=0.19 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from sympy->torch>=1.13.0->peft==0.7.2.dev0) (1.3.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install git+https://github.com/huggingface/peft@62237dc9b1e460825462d6b1e7b0cf3418cec357 --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers==4.36.2 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (4.36.2)\n",
      "Requirement already satisfied: datasets==2.16.1 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (2.16.1)\n",
      "Requirement already satisfied: accelerate==0.26.1 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (0.26.1)\n",
      "Requirement already satisfied: evaluate==0.4.1 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (0.4.1)\n",
      "Requirement already satisfied: bitsandbytes==0.42.0 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (0.42.0)\n",
      "Requirement already satisfied: filelock in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from transformers==4.36.2) (3.13.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from transformers==4.36.2) (0.20.2)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from transformers==4.36.2) (1.26.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from transformers==4.36.2) (21.3)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from transformers==4.36.2) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from transformers==4.36.2) (2023.12.25)\n",
      "Requirement already satisfied: requests in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from transformers==4.36.2) (2.31.0)\n",
      "Requirement already satisfied: tokenizers<0.19,>=0.14 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from transformers==4.36.2) (0.15.0)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from transformers==4.36.2) (0.4.1)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from transformers==4.36.2) (4.66.1)\n",
      "Requirement already satisfied: pyarrow>=8.0.0 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from datasets==2.16.1) (14.0.1)\n",
      "Requirement already satisfied: pyarrow-hotfix in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from datasets==2.16.1) (0.6)\n",
      "Requirement already satisfied: dill<0.3.8,>=0.3.0 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from datasets==2.16.1) (0.3.7)\n",
      "Requirement already satisfied: pandas in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from datasets==2.16.1) (2.1.3)\n",
      "Requirement already satisfied: xxhash in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from datasets==2.16.1) (3.4.1)\n",
      "Requirement already satisfied: multiprocess in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from datasets==2.16.1) (0.70.15)\n",
      "Requirement already satisfied: fsspec<=2023.10.0,>=2023.1.0 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from fsspec[http]<=2023.10.0,>=2023.1.0->datasets==2.16.1) (2023.10.0)\n",
      "Requirement already satisfied: aiohttp in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from datasets==2.16.1) (3.9.1)\n",
      "Requirement already satisfied: psutil in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from accelerate==0.26.1) (5.9.5)\n",
      "Requirement already satisfied: torch>=1.10.0 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from accelerate==0.26.1) (2.1.2)\n",
      "Requirement already satisfied: responses<0.19 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from evaluate==0.4.1) (0.18.0)\n",
      "Requirement already satisfied: scipy in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from bitsandbytes==0.42.0) (1.11.4)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from aiohttp->datasets==2.16.1) (23.1.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from aiohttp->datasets==2.16.1) (6.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from aiohttp->datasets==2.16.1) (1.9.4)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from aiohttp->datasets==2.16.1) (1.4.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from aiohttp->datasets==2.16.1) (1.3.1)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from aiohttp->datasets==2.16.1) (4.0.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers==4.36.2) (4.8.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from packaging>=20.0->transformers==4.36.2) (3.1.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from requests->transformers==4.36.2) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from requests->transformers==4.36.2) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from requests->transformers==4.36.2) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from requests->transformers==4.36.2) (2023.11.17)\n",
      "Requirement already satisfied: sympy in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from torch>=1.10.0->accelerate==0.26.1) (1.12)\n",
      "Requirement already satisfied: networkx in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from torch>=1.10.0->accelerate==0.26.1) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from torch>=1.10.0->accelerate==0.26.1) (3.1.2)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from torch>=1.10.0->accelerate==0.26.1) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from torch>=1.10.0->accelerate==0.26.1) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from torch>=1.10.0->accelerate==0.26.1) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from torch>=1.10.0->accelerate==0.26.1) (8.9.2.26)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from torch>=1.10.0->accelerate==0.26.1) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from torch>=1.10.0->accelerate==0.26.1) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from torch>=1.10.0->accelerate==0.26.1) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from torch>=1.10.0->accelerate==0.26.1) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from torch>=1.10.0->accelerate==0.26.1) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.18.1 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from torch>=1.10.0->accelerate==0.26.1) (2.18.1)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from torch>=1.10.0->accelerate==0.26.1) (12.1.105)\n",
      "Requirement already satisfied: triton==2.1.0 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from torch>=1.10.0->accelerate==0.26.1) (2.1.0)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.10.0->accelerate==0.26.1) (12.3.101)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from pandas->datasets==2.16.1) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from pandas->datasets==2.16.1) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from pandas->datasets==2.16.1) (2023.3)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets==2.16.1) (1.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from jinja2->torch>=1.10.0->accelerate==0.26.1) (2.1.3)\n",
      "Requirement already satisfied: mpmath>=0.19 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from sympy->torch>=1.10.0->accelerate==0.26.1) (1.3.0)\n",
      "Collecting git+https://github.com/huggingface/trl.git\n",
      "  Cloning https://github.com/huggingface/trl.git to /tmp/pip-req-build-3ygb97tw\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/huggingface/trl.git /tmp/pip-req-build-3ygb97tw\n",
      "  Resolved https://github.com/huggingface/trl.git to commit 928d14445e31b3586ce8b73ca70ecb02dc603369\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: torch>=1.4.0 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from trl==0.7.10.dev0) (2.1.2)\n",
      "Requirement already satisfied: transformers>=4.31.0 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from trl==0.7.10.dev0) (4.36.2)\n",
      "Requirement already satisfied: numpy>=1.18.2 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from trl==0.7.10.dev0) (1.26.2)\n",
      "Requirement already satisfied: accelerate in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from trl==0.7.10.dev0) (0.26.1)\n",
      "Requirement already satisfied: datasets in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from trl==0.7.10.dev0) (2.16.1)\n",
      "Requirement already satisfied: tyro>=0.5.11 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from trl==0.7.10.dev0) (0.6.3)\n",
      "Requirement already satisfied: filelock in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from torch>=1.4.0->trl==0.7.10.dev0) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from torch>=1.4.0->trl==0.7.10.dev0) (4.8.0)\n",
      "Requirement already satisfied: sympy in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from torch>=1.4.0->trl==0.7.10.dev0) (1.12)\n",
      "Requirement already satisfied: networkx in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from torch>=1.4.0->trl==0.7.10.dev0) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from torch>=1.4.0->trl==0.7.10.dev0) (3.1.2)\n",
      "Requirement already satisfied: fsspec in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from torch>=1.4.0->trl==0.7.10.dev0) (2023.10.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from torch>=1.4.0->trl==0.7.10.dev0) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from torch>=1.4.0->trl==0.7.10.dev0) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from torch>=1.4.0->trl==0.7.10.dev0) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from torch>=1.4.0->trl==0.7.10.dev0) (8.9.2.26)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from torch>=1.4.0->trl==0.7.10.dev0) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from torch>=1.4.0->trl==0.7.10.dev0) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from torch>=1.4.0->trl==0.7.10.dev0) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from torch>=1.4.0->trl==0.7.10.dev0) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from torch>=1.4.0->trl==0.7.10.dev0) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.18.1 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from torch>=1.4.0->trl==0.7.10.dev0) (2.18.1)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from torch>=1.4.0->trl==0.7.10.dev0) (12.1.105)\n",
      "Requirement already satisfied: triton==2.1.0 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from torch>=1.4.0->trl==0.7.10.dev0) (2.1.0)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.4.0->trl==0.7.10.dev0) (12.3.101)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from transformers>=4.31.0->trl==0.7.10.dev0) (0.20.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from transformers>=4.31.0->trl==0.7.10.dev0) (21.3)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from transformers>=4.31.0->trl==0.7.10.dev0) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from transformers>=4.31.0->trl==0.7.10.dev0) (2023.12.25)\n",
      "Requirement already satisfied: requests in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from transformers>=4.31.0->trl==0.7.10.dev0) (2.31.0)\n",
      "Requirement already satisfied: tokenizers<0.19,>=0.14 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from transformers>=4.31.0->trl==0.7.10.dev0) (0.15.0)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from transformers>=4.31.0->trl==0.7.10.dev0) (0.4.1)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from transformers>=4.31.0->trl==0.7.10.dev0) (4.66.1)\n",
      "Requirement already satisfied: docstring-parser>=0.14.1 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from tyro>=0.5.11->trl==0.7.10.dev0) (0.15)\n",
      "Requirement already satisfied: rich>=11.1.0 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from tyro>=0.5.11->trl==0.7.10.dev0) (13.7.0)\n",
      "Requirement already satisfied: shtab>=1.5.6 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from tyro>=0.5.11->trl==0.7.10.dev0) (1.6.5)\n",
      "Requirement already satisfied: psutil in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from accelerate->trl==0.7.10.dev0) (5.9.5)\n",
      "Requirement already satisfied: pyarrow>=8.0.0 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from datasets->trl==0.7.10.dev0) (14.0.1)\n",
      "Requirement already satisfied: pyarrow-hotfix in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from datasets->trl==0.7.10.dev0) (0.6)\n",
      "Requirement already satisfied: dill<0.3.8,>=0.3.0 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from datasets->trl==0.7.10.dev0) (0.3.7)\n",
      "Requirement already satisfied: pandas in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from datasets->trl==0.7.10.dev0) (2.1.3)\n",
      "Requirement already satisfied: xxhash in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from datasets->trl==0.7.10.dev0) (3.4.1)\n",
      "Requirement already satisfied: multiprocess in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from datasets->trl==0.7.10.dev0) (0.70.15)\n",
      "Requirement already satisfied: aiohttp in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from datasets->trl==0.7.10.dev0) (3.9.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from aiohttp->datasets->trl==0.7.10.dev0) (23.1.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from aiohttp->datasets->trl==0.7.10.dev0) (6.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from aiohttp->datasets->trl==0.7.10.dev0) (1.9.4)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from aiohttp->datasets->trl==0.7.10.dev0) (1.4.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from aiohttp->datasets->trl==0.7.10.dev0) (1.3.1)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from aiohttp->datasets->trl==0.7.10.dev0) (4.0.3)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from packaging>=20.0->transformers>=4.31.0->trl==0.7.10.dev0) (3.1.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from requests->transformers>=4.31.0->trl==0.7.10.dev0) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from requests->transformers>=4.31.0->trl==0.7.10.dev0) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from requests->transformers>=4.31.0->trl==0.7.10.dev0) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from requests->transformers>=4.31.0->trl==0.7.10.dev0) (2023.11.17)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from rich>=11.1.0->tyro>=0.5.11->trl==0.7.10.dev0) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from rich>=11.1.0->tyro>=0.5.11->trl==0.7.10.dev0) (2.17.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from jinja2->torch>=1.4.0->trl==0.7.10.dev0) (2.1.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from pandas->datasets->trl==0.7.10.dev0) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from pandas->datasets->trl==0.7.10.dev0) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from pandas->datasets->trl==0.7.10.dev0) (2023.3)\n",
      "Requirement already satisfied: mpmath>=0.19 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from sympy->torch>=1.4.0->trl==0.7.10.dev0) (1.3.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich>=11.1.0->tyro>=0.5.11->trl==0.7.10.dev0) (0.1.2)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets->trl==0.7.10.dev0) (1.16.0)\n",
      "Collecting git+https://github.com/huggingface/peft.git\n",
      "  Cloning https://github.com/huggingface/peft.git to /tmp/pip-req-build-xd4r0kl5\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/huggingface/peft.git /tmp/pip-req-build-xd4r0kl5\n",
      "  Resolved https://github.com/huggingface/peft.git to commit 62237dc9b1e460825462d6b1e7b0cf3418cec357\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from peft==0.7.2.dev0) (1.26.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from peft==0.7.2.dev0) (21.3)\n",
      "Requirement already satisfied: psutil in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from peft==0.7.2.dev0) (5.9.5)\n",
      "Requirement already satisfied: pyyaml in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from peft==0.7.2.dev0) (6.0.1)\n",
      "Requirement already satisfied: torch>=1.13.0 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from peft==0.7.2.dev0) (2.1.2)\n",
      "Requirement already satisfied: transformers in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from peft==0.7.2.dev0) (4.36.2)\n",
      "Requirement already satisfied: tqdm in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from peft==0.7.2.dev0) (4.66.1)\n",
      "Requirement already satisfied: accelerate>=0.21.0 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from peft==0.7.2.dev0) (0.26.1)\n",
      "Requirement already satisfied: safetensors in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from peft==0.7.2.dev0) (0.4.1)\n",
      "Requirement already satisfied: huggingface-hub>=0.17.0 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from peft==0.7.2.dev0) (0.20.2)\n",
      "Requirement already satisfied: filelock in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from huggingface-hub>=0.17.0->peft==0.7.2.dev0) (3.13.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from huggingface-hub>=0.17.0->peft==0.7.2.dev0) (2023.10.0)\n",
      "Requirement already satisfied: requests in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from huggingface-hub>=0.17.0->peft==0.7.2.dev0) (2.31.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from huggingface-hub>=0.17.0->peft==0.7.2.dev0) (4.8.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from packaging>=20.0->peft==0.7.2.dev0) (3.1.1)\n",
      "Requirement already satisfied: sympy in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from torch>=1.13.0->peft==0.7.2.dev0) (1.12)\n",
      "Requirement already satisfied: networkx in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from torch>=1.13.0->peft==0.7.2.dev0) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from torch>=1.13.0->peft==0.7.2.dev0) (3.1.2)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from torch>=1.13.0->peft==0.7.2.dev0) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from torch>=1.13.0->peft==0.7.2.dev0) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from torch>=1.13.0->peft==0.7.2.dev0) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from torch>=1.13.0->peft==0.7.2.dev0) (8.9.2.26)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from torch>=1.13.0->peft==0.7.2.dev0) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from torch>=1.13.0->peft==0.7.2.dev0) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from torch>=1.13.0->peft==0.7.2.dev0) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from torch>=1.13.0->peft==0.7.2.dev0) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from torch>=1.13.0->peft==0.7.2.dev0) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.18.1 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from torch>=1.13.0->peft==0.7.2.dev0) (2.18.1)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from torch>=1.13.0->peft==0.7.2.dev0) (12.1.105)\n",
      "Requirement already satisfied: triton==2.1.0 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from torch>=1.13.0->peft==0.7.2.dev0) (2.1.0)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.13.0->peft==0.7.2.dev0) (12.3.101)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from transformers->peft==0.7.2.dev0) (2023.12.25)\n",
      "Requirement already satisfied: tokenizers<0.19,>=0.14 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from transformers->peft==0.7.2.dev0) (0.15.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from jinja2->torch>=1.13.0->peft==0.7.2.dev0) (2.1.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from requests->huggingface-hub>=0.17.0->peft==0.7.2.dev0) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from requests->huggingface-hub>=0.17.0->peft==0.7.2.dev0) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from requests->huggingface-hub>=0.17.0->peft==0.7.2.dev0) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from requests->huggingface-hub>=0.17.0->peft==0.7.2.dev0) (2023.11.17)\n",
      "Requirement already satisfied: mpmath>=0.19 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from sympy->torch>=1.13.0->peft==0.7.2.dev0) (1.3.0)\n",
      "Requirement already satisfied: torch==2.1.2 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (2.1.2)\n",
      "Requirement already satisfied: tensorboard in /opt/conda/envs/pytorch/lib/python3.10/site-packages (2.15.1)\n",
      "Requirement already satisfied: filelock in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from torch==2.1.2) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from torch==2.1.2) (4.8.0)\n",
      "Requirement already satisfied: sympy in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from torch==2.1.2) (1.12)\n",
      "Requirement already satisfied: networkx in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from torch==2.1.2) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from torch==2.1.2) (3.1.2)\n",
      "Requirement already satisfied: fsspec in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from torch==2.1.2) (2023.10.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from torch==2.1.2) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from torch==2.1.2) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from torch==2.1.2) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from torch==2.1.2) (8.9.2.26)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from torch==2.1.2) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from torch==2.1.2) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from torch==2.1.2) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from torch==2.1.2) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from torch==2.1.2) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.18.1 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from torch==2.1.2) (2.18.1)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from torch==2.1.2) (12.1.105)\n",
      "Requirement already satisfied: triton==2.1.0 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from torch==2.1.2) (2.1.0)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch==2.1.2) (12.3.101)\n",
      "Requirement already satisfied: absl-py>=0.4 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from tensorboard) (2.0.0)\n",
      "Requirement already satisfied: grpcio>=1.48.2 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from tensorboard) (1.60.0)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from tensorboard) (2.26.2)\n",
      "Requirement already satisfied: google-auth-oauthlib<2,>=0.5 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from tensorboard) (1.2.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from tensorboard) (3.5.2)\n",
      "Requirement already satisfied: numpy>=1.12.0 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from tensorboard) (1.26.2)\n",
      "Requirement already satisfied: protobuf<4.24,>=3.19.6 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from tensorboard) (4.23.4)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from tensorboard) (2.31.0)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from tensorboard) (68.2.2)\n",
      "Requirement already satisfied: six>1.9 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from tensorboard) (1.16.0)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from tensorboard) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from tensorboard) (3.0.1)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard) (5.3.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard) (0.3.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard) (4.7.2)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from google-auth-oauthlib<2,>=0.5->tensorboard) (1.3.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard) (2023.11.17)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from werkzeug>=1.0.1->tensorboard) (2.1.3)\n",
      "Requirement already satisfied: mpmath>=0.19 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from sympy->torch==2.1.2) (1.3.0)\n",
      "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard) (0.5.1)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard) (3.2.2)\n"
     ]
    }
   ],
   "source": [
    "# Install Hugging Face libraries\n",
    "!pip install  --upgrade \\\n",
    "  \"transformers==4.36.2\" \\\n",
    "  \"datasets==2.16.1\" \\\n",
    "  \"accelerate==0.26.1\" \\\n",
    "  \"evaluate==0.4.1\" \\\n",
    "  \"bitsandbytes==0.42.0\" # \\\n",
    "  # \"peft==0.7.1\" \\\n",
    "  # \"trl==0.8.0\"\n",
    "\n",
    "# pinned peft & trl to commit until new release\n",
    "!pip install git+https://github.com/huggingface/trl@1f59eeb9bb4298aa6857754284d49cecc483ec36 --upgrade\n",
    "!pip install git+https://github.com/huggingface/peft@62237dc9b1e460825462d6b1e7b0cf3418cec357 --upgrade\n",
    "  \n",
    "# Install Pytorch & other libraries\n",
    "!pip install \"torch==2.1.2\" tensorboard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you are using a GPU with Ampere architecture (e.g. NVIDIA A10G or RTX 4090/3090) or newer you can use Flash attention. Flash Attention is a an method that reorders the attention computation and leverages classical techniques (tiling, recomputation) to significantly speed it up and reduce memory usage from quadratic to linear in sequence length. The TL;DR; accelerates training up to 3x. Learn more at [FlashAttention](https://github.com/Dao-AILab/flash-attention/tree/main).\n",
    "\n",
    "_Note: If your machine has less than 96GB of RAM and lots of CPU cores, reduce the number of `MAX_JOBS`. On the `g5.2xlarge` we used `4`._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch; assert torch.cuda.get_device_capability()[0] >= 8, 'Hardware not supported for Flash Attention'\n",
    "# install flash-attn\n",
    "!pip install ninja packaging\n",
    "!MAX_JOBS=4 pip install flash-attn --no-build-isolation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Installing flash attention can take quite a bit of time (10-45 minutes)._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the [Hugging Face Hub](https://huggingface.co/models) as a remote model versioning service. This means we will automatically push our model, logs and information to the Hub during training. You must register on the [Hugging Face](https://huggingface.co/join) for this. After you have an account, we will use the `login` util from the `huggingface_hub` package to log into our account and store our token (access key) on the disk.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "\n",
    "login(\n",
    "  token=\"\", # ADD YOUR TOKEN HERE\n",
    "  add_to_git_credential=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Create and prepare the dataset\n",
    "\n",
    "Once you have determined that fine-tuning is the right solution we need to create a dataset to fine-tune our model. The dataset should be a diverse set of demonstrations of the task you want to solve. There are several ways to create such a dataset, including:\n",
    "* Using existing open-source datasets, e.g., [Spider](https://huggingface.co/datasets/spider)\n",
    "* Using LLMs to create synthetically datasets, e.g., [Alpaca](https://huggingface.co/datasets/tatsu-lab/alpaca)\n",
    "* Using Humans to create datasets, e.g., [Dolly](https://huggingface.co/datasets/databricks/databricks-dolly-15k).\n",
    "* Using a combination of the above methods, e.g., [Orca](https://huggingface.co/datasets/Open-Orca/OpenOrca)\n",
    "\n",
    "Each of the methods has its own advantages and disadvantages and depends on the budget, time, and quality requirements. For example, using an existing dataset is the easiest but might not be tailored to your specific use case, while using humans might be the most accurate but can be time-consuming and expensive. It is also possible to combine several methods to create an instruction dataset, as shown in [Orca: Progressive Learning from Complex Explanation Traces of GPT-4.](https://arxiv.org/abs/2306.02707)\n",
    "\n",
    "In our example we will use an already existing dataset called [sql-create-context](https://huggingface.co/datasets/b-mc2/sql-create-context), which contains samples of natural language instructions, schema definitions and the corresponding SQL query.\n",
    "\n",
    "With the latest release of `trl` we now support popular instruction and conversation dataset formats. This means we only need to convert our dataset to one of the supported formats and `trl` will take care of the rest. Those formats include:\n",
    "* conversational format\n",
    "```json\n",
    "{\"messages\": [{\"role\": \"system\", \"content\": \"You are...\"}, {\"role\": \"user\", \"content\": \"...\"}, {\"role\": \"assistant\", \"content\": \"...\"}]}\n",
    "{\"messages\": [{\"role\": \"system\", \"content\": \"You are...\"}, {\"role\": \"user\", \"content\": \"...\"}, {\"role\": \"assistant\", \"content\": \"...\"}]}\n",
    "{\"messages\": [{\"role\": \"system\", \"content\": \"You are...\"}, {\"role\": \"user\", \"content\": \"...\"}, {\"role\": \"assistant\", \"content\": \"...\"}]}\n",
    "```\n",
    "* instruction format\n",
    "\n",
    "```json\n",
    "{\"prompt\": \"<prompt text>\", \"completion\": \"<ideal generated text>\"}\n",
    "{\"prompt\": \"<prompt text>\", \"completion\": \"<ideal generated text>\"}\n",
    "{\"prompt\": \"<prompt text>\", \"completion\": \"<ideal generated text>\"}\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our example we are going to load our open-source dataset using the 🤗 Datasets library and then convert it into the the conversational format, where we include the schema definition in the system message for our assistant. We'll then save the dataset as jsonl file, which we can then use to fine-tune our model. We are randomly downsampling the dataset to only 10,000 samples.\n",
    "\n",
    "_Note: This step can be different for your use case. For example, if you have already a dataset from, e.g. working with OpenAI, you can skip this step and go directly to the fine-tuning step._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "963395499d8a4574947ea4b6e671c05d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/12500 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'content': 'You are an text to SQL query translator. Users will ask you questions in English and you will generate a SQL query based on the provided SCHEMA.\\nSCHEMA:\\nCREATE TABLE table_name_98 (margin_of_victory VARCHAR, tournament VARCHAR)', 'role': 'system'}, {'content': 'What is the margin of victory in WGC-Accenture Match Play Championship?', 'role': 'user'}, {'content': 'SELECT margin_of_victory FROM table_name_98 WHERE tournament = \"wgc-accenture match play championship\"', 'role': 'assistant'}]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ab056472b438490586752c8c48904e6f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating json from Arrow format:   0%|          | 0/10 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ac8d393a3434d2b80d72cd575a45633",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating json from Arrow format:   0%|          | 0/3 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "1189023"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Convert dataset to OAI messages\n",
    "system_message = \"\"\"You are an text to SQL query translator. Users will ask you questions in English and you will generate a SQL query based on the provided SCHEMA.\n",
    "SCHEMA:\n",
    "{schema}\"\"\"\n",
    "\n",
    "def create_conversation(sample):\n",
    "  return {\n",
    "    \"messages\": [\n",
    "      {\"role\": \"system\", \"content\": system_message.format(schema=sample[\"context\"])},\n",
    "      {\"role\": \"user\", \"content\": sample[\"question\"]},\n",
    "      {\"role\": \"assistant\", \"content\": sample[\"answer\"]}\n",
    "    ]\n",
    "  }  \n",
    "\n",
    "# Load dataset from the hub\n",
    "dataset = load_dataset(\"b-mc2/sql-create-context\", split=\"train\")\n",
    "dataset = dataset.shuffle().select(range(12500))\n",
    "\n",
    "# Convert dataset to OAI messages\n",
    "dataset = dataset.map(create_conversation, remove_columns=dataset.features,batched=False)\n",
    "# split dataset into 10,000 training samples and 2,500 test samples\n",
    "dataset = dataset.train_test_split(test_size=2500/12500)\n",
    "\n",
    "print(dataset[\"train\"][345][\"messages\"])\n",
    "\n",
    "# save datasets to disk \n",
    "dataset[\"train\"].to_json(\"train_dataset.json\", orient=\"records\")\n",
    "dataset[\"test\"].to_json(\"test_dataset.json\", orient=\"records\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Fine-tune LLM using `trl` and the `SFTTrainer` \n",
    "\n",
    "We are now ready to fine-tune our model. We will use the [SFTTrainer](https://huggingface.co/docs/trl/sft_trainer) from `trl` to fine-tune our model. The `SFTTrainer` makes it straightfoward to supervise fine-tune open LLMs. The `SFTTrainer` is a subclass of the `Trainer` from the `transformers` library and supports all the same features, including logging, evaluation, and checkpointing, but adds additiional quality of life features, including:\n",
    "* Dataset formatting, including conversational and instruction format\n",
    "* Training on completions only, ignoring prompts\n",
    "* Packing datasets for more efficient training\n",
    "* PEFT (parameter-efficient fine-tuning) support including Q-LoRA\n",
    "* Preparing the model and tokenizer for conversational fine-tuning (e.g. adding special tokens)\n",
    " \n",
    "We will use the dataset formatting, packing and PEFT features in our example. As peft method we will use [QLoRA](https://arxiv.org/abs/2305.14314) a technique to reduce the memory footprint of large language models during finetuning, without sacrificing performance by using quantization. If you want to learn more about QLoRA and how it works, check out [Making LLMs even more accessible with bitsandbytes, 4-bit quantization and QLoRA](https://huggingface.co/blog/4bit-transformers-bitsandbytes) blog post.\n",
    "\n",
    "Now, lets get started! 🚀 \n",
    "\n",
    "First, we need to load our dataset from disk. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Load jsonl data from disk\n",
    "dataset = load_dataset(\"json\", data_files=\"train_dataset.json\", split=\"train\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will load our LLM. For our use case we are going to use CodeLlama 7B. CodeLlama is a Llama model trained for general code synthesis and understanding. \n",
    "But we can easily swap out the model for another model, e.g. [Mistral](https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.2) or [Mixtral](https://huggingface.co/mistralai/Mixtral-8x7B-Instruct-v0.1) models, TII [Falcon](https://huggingface.co/tiiuae/falcon-40b), or any other LLMs by changing our `model_id` variable. We will use bitsandbytes to quantize our model to 4-bit.\n",
    "\n",
    "_Note: Be aware the bigger the model the more memory it will require. In our example we will use the 7B version, which can be tuned on 24GB GPUs. If you have a smaller GPU._\n",
    "\n",
    "Correctly, preparing the LLM and Tokenizer for training chat/conversational models is crucial. We need to add new special tokens to the tokenizer and model and teach to understand the different roles in a conversation. In `trl` we have a convinient method called [setup_chat_format](https://huggingface.co/docs/trl/main/en/sft_trainer#add-special-tokens-for-chat-format), which:\n",
    "* Adds special tokens to the tokenizer, e.g. `<|im_start|>` and `<|im_end|>`, to indicate the start and end of a conversation.\n",
    "* Resizes the model’s embedding layer to accommodate the new tokens.\n",
    "* Sets the `chat_template` of the tokenizer, which is used to format the input data into a chat-like format. The default is `chatml` from OpenAI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f654a51ffa7d4e429f5f526166b70af6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "from trl import setup_chat_format\n",
    "\n",
    "# Hugging Face model id\n",
    "model_id = \"codellama/CodeLlama-7b-hf\" # or `mistralai/Mistral-7B-v0.1`\n",
    "\n",
    "# BitsAndBytesConfig int-4 config\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True, bnb_4bit_use_double_quant=True, bnb_4bit_quant_type=\"nf4\", bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "# Load model and tokenizer\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    device_map=\"auto\",\n",
    "    attn_implementation=\"flash_attention_2\",\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    quantization_config=bnb_config\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "tokenizer.padding_side = 'right' # to prevent warnings\n",
    "\n",
    "# set chat template to OAI chatML, remove if you start from a fine-tuned model\n",
    "model, tokenizer = setup_chat_format(model, tokenizer)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `SFTTrainer`  supports a native integration with `peft`, which makes it super easy to efficiently tune LLMs using, e.g. QLoRA. We only need to create our `LoraConfig` and provide it to the trainer. Our `LoraConfig` parameters are defined based on the [qlora paper](https://arxiv.org/pdf/2305.14314.pdf) and sebastian's [blog post](https://magazine.sebastianraschka.com/p/practical-tips-for-finetuning-llms)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import LoraConfig\n",
    "\n",
    "# LoRA config based on QLoRA paper & Sebastian Raschka experiment\n",
    "peft_config = LoraConfig(\n",
    "        lora_alpha=128,\n",
    "        lora_dropout=0.05,\n",
    "        r=256,\n",
    "        bias=\"none\",\n",
    "        target_modules=\"all-linear\",\n",
    "        task_type=\"CAUSAL_LM\", \n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we can start our training we need to define the hyperparameters (`TrainingArguments`) we want to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "args = TrainingArguments(\n",
    "    output_dir=\"code-llama-7b-text-to-sql\", # directory to save and repository id\n",
    "    num_train_epochs=2,                     # number of training epochs\n",
    "    per_device_train_batch_size=3,          # batch size per device during training\n",
    "    gradient_accumulation_steps=2,          # number of steps before performing a backward/update pass\n",
    "    gradient_checkpointing=True,            # use gradient checkpointing to save memory\n",
    "    optim=\"adamw_torch_fused\",              # use fused adamw optimizer\n",
    "    logging_steps=10,                       # log every 10 steps\n",
    "    save_strategy=\"epoch\",                  # save checkpoint every epoch\n",
    "    learning_rate=2e-4,                     # learning rate, based on QLoRA paper\n",
    "    bf16=True,                              # use bfloat16 precision\n",
    "    tf32=True,                              # use tf32 precision\n",
    "    max_grad_norm=0.3,                      # max gradient norm based on QLoRA paper\n",
    "    warmup_ratio=0.03,                      # warmup ratio based on QLoRA paper\n",
    "    lr_scheduler_type=\"constant\",           # use constant learning rate scheduler\n",
    "    push_to_hub=False,                       # push model to hub\n",
    "    report_to=\"tensorboard\",                # report metrics to tensorboard\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have every building block we need to create our `SFTTrainer` to start then training our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trl import SFTTrainer\n",
    "\n",
    "max_seq_length = 3072 # max sequence length for model and packing of the dataset\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=dataset,\n",
    "    peft_config=peft_config,\n",
    "    max_seq_length=max_seq_length,\n",
    "    tokenizer=tokenizer,\n",
    "    packing=True,\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start training our model by calling the `train()` method on our `Trainer` instance. This will start the training loop and train our model for 3 epochs. Since we are using a PEFT method, we will only save the adapted model weights and not the full model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a CodeLlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "The input hidden states seems to be silently casted in float32, this might be related to the fact you have upcasted embedding or layer norm layers in float32. We will cast back the input in torch.bfloat16.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='136' max='136' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [136/136 58:45, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>1.109900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.685100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.579900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>0.526700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.508000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>0.493700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>0.478500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>0.453100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>0.442100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.445700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>110</td>\n",
       "      <td>0.439700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>0.440300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>130</td>\n",
       "      <td>0.432900</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/pytorch/lib/python3.10/site-packages/peft/utils/save_and_load.py:140: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n",
      "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/opt/conda/envs/pytorch/lib/python3.10/site-packages/peft/utils/save_and_load.py:140: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=136, training_loss=0.5363549344679889, metrics={'train_runtime': 3552.5084, 'train_samples_per_second': 0.229, 'train_steps_per_second': 0.038, 'total_flos': 1.0846501259024794e+17, 'train_loss': 0.5363549344679889, 'epoch': 2.0})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# start training, the model will be automatically saved to the hub and the output directory\n",
    "trainer.train()\n",
    "\n",
    "# save model \n",
    "trainer.save_model()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training with Flash Attention for 3 epochs with a dataset of 10k samples took 02:08:00 on a `g5.2xlarge`. The instance costs `1,212$/h` which brings us to a total cost of `2.6$`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# free the memory again\n",
    "del model\n",
    "del trainer\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _Optional: Merge LoRA adapter in to the original model_\n",
    "\n",
    "When using QLoRA, we only train adapters and not the full model. This means when saving the model during training we only save the adapter weights and not the full model. If you want to save the full model, which makes it easier to use with Text Generation Inference you can merge the adapter weights into the model weights using the `merge_and_unload` method and then save the model with the `save_pretrained` method. This will save a default model, which can be used for inference.\n",
    "\n",
    "_Note: You might require > 30GB CPU Memory._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from peft import PeftModel, PeftConfig\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from peft import AutoPeftModelForCausalLM\n",
    "\n",
    "# Load PEFT model on CPU\n",
    "config = PeftConfig.from_pretrained(args.output_dir)\n",
    "model = AutoModelForCausalLM.from_pretrained(config.base_model_name_or_path,low_cpu_mem_usage=True)\n",
    "tokenizer = AutoTokenizer.from_pretrained(args.output_dir)\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "model = PeftModel.from_pretrained(model, args.output_dir)\n",
    "model = AutoPeftModelForCausalLM.from_pretrained(\n",
    "    args.output_dir,\n",
    "    torch_dtype=torch.float16,\n",
    "    low_cpu_mem_usage=True,\n",
    ")  \n",
    "# Merge LoRA and base model and save\n",
    "merged_model = model.merge_and_unload()\n",
    "merged_model.save_pretrained(args.output_dir,safe_serialization=True, max_shard_size=\"2GB\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Test Model and run Inference\n",
    "\n",
    "After the training is done we want to evaluate and test our model. We will load different samples from the original dataset and evaluate the model on those samples, using a simple loop and accuracy as our metric. \n",
    "\n",
    "_Note: Evaluating Generative AI models is not a trivial task since 1 input can have multiple correct outputs. If you want to learn more about evaluating generative models, check out [Evaluate LLMs and RAG a practical example using Langchain and Hugging Face](https://www.philschmid.de/evaluate-llm) blog post._\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "28c4459b207e4a72a87962d972a42e66",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The model 'PeftModelForCausalLM' is not supported for text-generation. Supported models are ['BartForCausalLM', 'BertLMHeadModel', 'BertGenerationDecoder', 'BigBirdForCausalLM', 'BigBirdPegasusForCausalLM', 'BioGptForCausalLM', 'BlenderbotForCausalLM', 'BlenderbotSmallForCausalLM', 'BloomForCausalLM', 'CamembertForCausalLM', 'LlamaForCausalLM', 'CodeGenForCausalLM', 'CpmAntForCausalLM', 'CTRLLMHeadModel', 'Data2VecTextForCausalLM', 'ElectraForCausalLM', 'ErnieForCausalLM', 'FalconForCausalLM', 'FuyuForCausalLM', 'GitForCausalLM', 'GPT2LMHeadModel', 'GPT2LMHeadModel', 'GPTBigCodeForCausalLM', 'GPTNeoForCausalLM', 'GPTNeoXForCausalLM', 'GPTNeoXJapaneseForCausalLM', 'GPTJForCausalLM', 'LlamaForCausalLM', 'MarianForCausalLM', 'MBartForCausalLM', 'MegaForCausalLM', 'MegatronBertForCausalLM', 'MistralForCausalLM', 'MixtralForCausalLM', 'MptForCausalLM', 'MusicgenForCausalLM', 'MvpForCausalLM', 'OpenLlamaForCausalLM', 'OpenAIGPTLMHeadModel', 'OPTForCausalLM', 'PegasusForCausalLM', 'PersimmonForCausalLM', 'PhiForCausalLM', 'PLBartForCausalLM', 'ProphetNetForCausalLM', 'QDQBertLMHeadModel', 'ReformerModelWithLMHead', 'RemBertForCausalLM', 'RobertaForCausalLM', 'RobertaPreLayerNormForCausalLM', 'RoCBertForCausalLM', 'RoFormerForCausalLM', 'RwkvForCausalLM', 'Speech2Text2ForCausalLM', 'TransfoXLLMHeadModel', 'TrOCRForCausalLM', 'WhisperForCausalLM', 'XGLMForCausalLM', 'XLMWithLMHeadModel', 'XLMProphetNetForCausalLM', 'XLMRobertaForCausalLM', 'XLMRobertaXLForCausalLM', 'XLNetLMHeadModel', 'XmodForCausalLM'].\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from peft import PeftModel, PeftConfig\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline \n",
    "\n",
    "peft_model_id = \"/home/ubuntu/deep-learning-pytorch-huggingface/training/code-llama-7b-text-to-sql/checkpoint-136\"\n",
    "# peft_model_id = args.output_dir\n",
    "\n",
    "# Load Model with PEFT adapter\n",
    "tokenizer = AutoTokenizer.from_pretrained(peft_model_id)\n",
    "config = PeftConfig.from_pretrained(peft_model_id)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "  config.base_model_name_or_path,\n",
    "  device_map=\"auto\",\n",
    "  torch_dtype=torch.float16\n",
    ")\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "model = PeftModel.from_pretrained(model, peft_model_id)\n",
    "# load into pipeline\n",
    "pipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s load our test dataset try to generate an instruction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/pytorch/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:389: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/opt/conda/envs/pytorch/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:394: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query:\n",
      "In the Tournament during which Jiří Novák made it to the 1st round(1R) in 1995, how did he do in 2001?\n",
      "Original Answer:\n",
      "SELECT 2001 FROM table_name_32 WHERE 1995 = \"1r\"\n",
      "Generated Answer:\n",
      "SELECT 2001 FROM table_name_32 WHERE 1995 = \"1r\" AND 2009 = \"1r\" AND 2011 = \"1r\" AND 2013 = \"1r\" AND 2015 = \"1r\" AND 2007 = \"1r\" AND 2017 = \"1r\" AND 2019 = \"1r\" AND 2003 = \"1r\" AND 2005 = \"1r\" AND 2009 = \"1r\" AND 2011 = \"1r\" AND 2013 = \"1r\" AND 2015 = \"1r\" AND 2007 = \"1r\" AND 2017 = \"1r\" AND 2019 = \"1r\" AND 2003 = \"1r\" AND 2005 = \"1r\" AND 2009 = \"1r\" AND 2011 = \"1r\" AND 2013 = \"1r\" AND\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset \n",
    "from random import randint\n",
    "\n",
    "\n",
    "# Load our test dataset\n",
    "eval_dataset = load_dataset(\"json\", data_files=\"test_dataset.json\", split=\"train\")\n",
    "rand_idx = randint(0, len(eval_dataset))\n",
    "\n",
    "# Test on sample \n",
    "prompt = pipe.tokenizer.apply_chat_template(eval_dataset[rand_idx][\"messages\"][:2], tokenize=False, add_generation_prompt=True)\n",
    "outputs = pipe(prompt, max_new_tokens=256, do_sample=False, temperature=0.1, top_k=50, top_p=0.1)\n",
    "\n",
    "print(f\"Query:\\n{eval_dataset[rand_idx]['messages'][1]['content']}\")\n",
    "print(f\"Original Answer:\\n{eval_dataset[rand_idx]['messages'][2]['content']}\")\n",
    "print(f\"Generated Answer:\\n{outputs[0]['generated_text'][len(prompt):].strip()}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nice! Our model was able to generate a SQL query based on the natural language instruction. Lets evaluate our model on the full 2,500 samples of our test dataset. \n",
    "_Note: As mentioned above, evaluating generative models is not a trivial task. In our example we used the accuracy of the generated SQL based on the ground truth SQL query as our metric. An alternative way could be to automatically execute the generated SQL query and compare the results with the ground truth. This would be a more accurate metric but requires more work to setup._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "success_rate = []\n",
    "\n",
    "def evaluate(sample):\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": system_message.format(schema=sample[\"context\"])},\n",
    "        {\"role\": \"user\", \"content\":sample[\"question\"]},                \n",
    "    ]\n",
    "    # Test on sample \n",
    "    prompt = pipe.tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "    outputs = pipe(prompt, max_new_tokens=256, do_sample=True, temperature=0.7, top_k=50, top_p=0.95)\n",
    "    predicted_answer = outputs[0]['generated_text'][len(prompt):].strip()\n",
    "    if predicted_answer == sample[\"answer\"]:\n",
    "        success_rate.append(1)\n",
    "    else:\n",
    "        success_rate.append(0)\n",
    "\n",
    "# iterate over eval dataset and predict\n",
    "eval_res = eval_dataset.map(evaluate)  \n",
    "\n",
    "# compute accuracy\n",
    "accuracy = sum(success_rate)/len(success_rate)\n",
    "\n",
    "print(f\"Accuracy: {accuracy:.2f}\")  \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Deploy the LLM for Production\n",
    "\n",
    "You can now deploy your model to production. For deploying open LLMs into production we recommend using Text Generation Inference...\n",
    "\n",
    "\n",
    "\n",
    " There are several ways to deploy your model, including:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "2d58e898dde0263bc564c6968b04150abacfd33eed9b19aaa8e45c040360e146"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
