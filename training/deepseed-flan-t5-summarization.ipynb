{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!sudo apt install git-lfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: transformers==4.26.0 in /home/ubuntu/.local/lib/python3.8/site-packages (4.26.0)\n",
      "Requirement already satisfied: datasets==2.9.0 in /home/ubuntu/.local/lib/python3.8/site-packages (2.9.0)\n",
      "Requirement already satisfied: accelerate==0.16.0 in /home/ubuntu/.local/lib/python3.8/site-packages (0.16.0)\n",
      "Requirement already satisfied: evaluate==0.4.0 in /home/ubuntu/.local/lib/python3.8/site-packages (0.4.0)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /home/ubuntu/.local/lib/python3.8/site-packages (from transformers==4.26.0) (0.13.2)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.11.0 in /home/ubuntu/.local/lib/python3.8/site-packages (from transformers==4.26.0) (0.12.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/ubuntu/.local/lib/python3.8/site-packages (from transformers==4.26.0) (1.24.1)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from transformers==4.26.0) (3.9.0)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from transformers==4.26.0) (2.28.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.8/dist-packages (from transformers==4.26.0) (5.4.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/ubuntu/.local/lib/python3.8/site-packages (from transformers==4.26.0) (2022.10.31)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.8/dist-packages (from transformers==4.26.0) (21.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.8/dist-packages (from transformers==4.26.0) (4.64.1)\n",
      "Requirement already satisfied: fsspec[http]>=2021.11.1 in /usr/local/lib/python3.8/dist-packages (from datasets==2.9.0) (2022.11.0)\n",
      "Requirement already satisfied: dill<0.3.7 in /usr/local/lib/python3.8/dist-packages (from datasets==2.9.0) (0.3.6)\n",
      "Requirement already satisfied: multiprocess in /home/ubuntu/.local/lib/python3.8/site-packages (from datasets==2.9.0) (0.70.14)\n",
      "Requirement already satisfied: pandas in /home/ubuntu/.local/lib/python3.8/site-packages (from datasets==2.9.0) (1.5.3)\n",
      "Requirement already satisfied: xxhash in /home/ubuntu/.local/lib/python3.8/site-packages (from datasets==2.9.0) (3.2.0)\n",
      "Requirement already satisfied: pyarrow>=6.0.0 in /home/ubuntu/.local/lib/python3.8/site-packages (from datasets==2.9.0) (11.0.0)\n",
      "Requirement already satisfied: aiohttp in /usr/local/lib/python3.8/dist-packages (from datasets==2.9.0) (3.8.3)\n",
      "Requirement already satisfied: responses<0.19 in /home/ubuntu/.local/lib/python3.8/site-packages (from datasets==2.9.0) (0.18.0)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.8/dist-packages (from accelerate==0.16.0) (5.9.4)\n",
      "Requirement already satisfied: torch>=1.4.0 in /home/ubuntu/.local/lib/python3.8/site-packages (from accelerate==0.16.0) (1.13.1+cu116)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets==2.9.0) (4.0.2)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets==2.9.0) (1.3.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets==2.9.0) (6.0.4)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets==2.9.0) (1.3.3)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets==2.9.0) (22.2.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets==2.9.0) (1.8.2)\n",
      "Requirement already satisfied: charset-normalizer<3.0,>=2.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets==2.9.0) (2.1.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.8/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers==4.26.0) (4.4.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging>=20.0->transformers==4.26.0) (3.0.9)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->transformers==4.26.0) (1.26.13)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->transformers==4.26.0) (2022.12.7)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->transformers==4.26.0) (3.4)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.8/dist-packages (from pandas->datasets==2.9.0) (2022.7)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.8/dist-packages (from pandas->datasets==2.9.0) (2.8.2)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.8/dist-packages (from python-dateutil>=2.8.1->pandas->datasets==2.9.0) (1.16.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3.8 -m pip install --upgrade pip\u001b[0m\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: deepspeed==0.8.0 in /home/ubuntu/.local/lib/python3.8/site-packages (0.8.0)\n",
      "Requirement already satisfied: ninja in /home/ubuntu/.local/lib/python3.8/site-packages (1.11.1)\n",
      "Requirement already satisfied: numpy in /home/ubuntu/.local/lib/python3.8/site-packages (from deepspeed==0.8.0) (1.24.1)\n",
      "Requirement already satisfied: pydantic in /usr/local/lib/python3.8/dist-packages (from deepspeed==0.8.0) (1.10.4)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.8/dist-packages (from deepspeed==0.8.0) (4.64.1)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.8/dist-packages (from deepspeed==0.8.0) (5.9.4)\n",
      "Requirement already satisfied: hjson in /home/ubuntu/.local/lib/python3.8/site-packages (from deepspeed==0.8.0) (3.1.0)\n",
      "Requirement already satisfied: torch in /home/ubuntu/.local/lib/python3.8/site-packages (from deepspeed==0.8.0) (1.13.1+cu116)\n",
      "Requirement already satisfied: py-cpuinfo in /home/ubuntu/.local/lib/python3.8/site-packages (from deepspeed==0.8.0) (9.0.0)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.8/dist-packages (from deepspeed==0.8.0) (21.3)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging->deepspeed==0.8.0) (3.0.9)\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.8/dist-packages (from pydantic->deepspeed==0.8.0) (4.4.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3.8 -m pip install --upgrade pip\u001b[0m\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: rouge-score in /home/ubuntu/.local/lib/python3.8/site-packages (0.1.2)\n",
      "Requirement already satisfied: nltk in /home/ubuntu/.local/lib/python3.8/site-packages (3.8.1)\n",
      "Requirement already satisfied: py7zr in /home/ubuntu/.local/lib/python3.8/site-packages (0.20.2)\n",
      "Requirement already satisfied: tensorboard in /home/ubuntu/.local/lib/python3.8/site-packages (2.11.2)\n",
      "Requirement already satisfied: absl-py in /home/ubuntu/.local/lib/python3.8/site-packages (from rouge-score) (1.4.0)\n",
      "Requirement already satisfied: numpy in /home/ubuntu/.local/lib/python3.8/site-packages (from rouge-score) (1.24.1)\n",
      "Requirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.8/dist-packages (from rouge-score) (1.16.0)\n",
      "Requirement already satisfied: click in /usr/local/lib/python3.8/dist-packages (from nltk) (8.1.3)\n",
      "Requirement already satisfied: joblib in /home/ubuntu/.local/lib/python3.8/site-packages (from nltk) (1.2.0)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.8/dist-packages (from nltk) (4.64.1)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /home/ubuntu/.local/lib/python3.8/site-packages (from nltk) (2022.10.31)\n",
      "Requirement already satisfied: pyzstd>=0.14.4 in /home/ubuntu/.local/lib/python3.8/site-packages (from py7zr) (0.15.3)\n",
      "Requirement already satisfied: pyppmd<1.1.0,>=0.18.1 in /home/ubuntu/.local/lib/python3.8/site-packages (from py7zr) (1.0.0)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.8/dist-packages (from py7zr) (5.9.4)\n",
      "Requirement already satisfied: pycryptodomex>=3.6.6 in /home/ubuntu/.local/lib/python3.8/site-packages (from py7zr) (3.17)\n",
      "Requirement already satisfied: texttable in /home/ubuntu/.local/lib/python3.8/site-packages (from py7zr) (1.6.7)\n",
      "Requirement already satisfied: brotli>=1.0.9 in /home/ubuntu/.local/lib/python3.8/site-packages (from py7zr) (1.0.9)\n",
      "Requirement already satisfied: multivolumefile>=0.2.3 in /home/ubuntu/.local/lib/python3.8/site-packages (from py7zr) (0.2.3)\n",
      "Requirement already satisfied: inflate64>=0.3.1 in /home/ubuntu/.local/lib/python3.8/site-packages (from py7zr) (0.3.1)\n",
      "Requirement already satisfied: pybcj>=0.6.0 in /home/ubuntu/.local/lib/python3.8/site-packages (from py7zr) (1.0.1)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /home/ubuntu/.local/lib/python3.8/site-packages (from tensorboard) (3.4.1)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.8/dist-packages (from tensorboard) (2.28.1)\n",
      "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.8/dist-packages (from tensorboard) (0.38.4)\n",
      "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /home/ubuntu/.local/lib/python3.8/site-packages (from tensorboard) (0.6.1)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /home/ubuntu/.local/lib/python3.8/site-packages (from tensorboard) (0.4.6)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in /home/ubuntu/.local/lib/python3.8/site-packages (from tensorboard) (2.16.0)\n",
      "Requirement already satisfied: protobuf<4,>=3.9.2 in /home/ubuntu/.local/lib/python3.8/site-packages (from tensorboard) (3.20.3)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.8/dist-packages (from tensorboard) (65.6.3)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /home/ubuntu/.local/lib/python3.8/site-packages (from tensorboard) (1.8.1)\n",
      "Requirement already satisfied: grpcio>=1.24.3 in /home/ubuntu/.local/lib/python3.8/site-packages (from tensorboard) (1.51.1)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.8/dist-packages (from tensorboard) (2.2.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.8/dist-packages (from google-auth<3,>=1.6.3->tensorboard) (0.2.8)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.8/dist-packages (from google-auth<3,>=1.6.3->tensorboard) (4.7.2)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /home/ubuntu/.local/lib/python3.8/site-packages (from google-auth<3,>=1.6.3->tensorboard) (5.3.0)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /home/ubuntu/.local/lib/python3.8/site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard) (1.3.1)\n",
      "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.8/dist-packages (from markdown>=2.6.8->tensorboard) (6.0.0)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /usr/local/lib/python3.8/dist-packages (from requests<3,>=2.21.0->tensorboard) (2.1.1)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests<3,>=2.21.0->tensorboard) (1.26.13)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests<3,>=2.21.0->tensorboard) (2022.12.7)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests<3,>=2.21.0->tensorboard) (3.4)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.8/dist-packages (from werkzeug>=1.0.1->tensorboard) (2.1.1)\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.8/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard) (3.11.0)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.8/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard) (0.4.8)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /usr/lib/python3/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard) (3.1.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3.8 -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install \"transformers==4.26.0\" \"datasets==2.9.0\" \"accelerate==0.16.0\" \"evaluate==0.4.0\" --upgrade\n",
    "!pip install \"deepspeed==0.8.0\" ninja --upgrade\n",
    "!pip install rouge-score nltk py7zr tensorboard\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Installed CUDA version 11.0 does not match the version torch was compiled with 11.6 but since the APIs are compatible, accepting this combination\n",
      "Time to load cpu_adam op: 2.3554303646087646 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using /home/ubuntu/.cache/torch_extensions/py38_cu116 as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module cpu_adam, skipping build step...\n",
      "Loading extension module cpu_adam...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<module 'cpu_adam' from '/home/ubuntu/.cache/torch_extensions/py38_cu116/cpu_adam/cpu_adam.so'>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import deepspeed\n",
    "deepspeed.ops.op_builder.CPUAdamBuilder().load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/bin/python3: No module named ds_report\n"
     ]
    }
   ],
   "source": [
    "!python3 -m ds_report"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# experiment config\n",
    "model_id = \"google/flan-t5-base\"\n",
    "repository_id = \"flan-t5-base-cnn\"\n",
    "\n",
    "# Dataset \n",
    "dataset_id = \"cnn_dailymail\"\n",
    "dataset_config = \"3.0.0\"\n",
    "save_dataset_path = \"data\"\n",
    "text_column = \"article\"\n",
    "summary_column = \"highlights\"\n",
    "prompt_start = \"Summarize the following news article:\\n\"\n",
    "generation_start = \"\\nSummary:\\n\"\n",
    "prompt_template = f\"{prompt_start}{{input}}{generation_start}\"\n",
    "\n",
    "max_source_length=500\n",
    "max_target_length=129"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading builder script: 100%|██████████| 8.33k/8.33k [00:00<00:00, 5.79MB/s]\n",
      "Downloading metadata: 100%|██████████| 9.88k/9.88k [00:00<00:00, 6.98MB/s]\n",
      "Downloading readme: 100%|██████████| 15.1k/15.1k [00:00<00:00, 9.46MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset cnn_dailymail/3.0.0 to /home/ubuntu/.cache/huggingface/datasets/cnn_dailymail/3.0.0/3.0.0/1b3c71476f6d152c31c1730e83ccb08bcf23e348233f4fcc11e182248e6bf7de...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading data: 100%|██████████| 159M/159M [00:02<00:00, 65.4MB/s]\n",
      "Downloading data: 100%|██████████| 376M/376M [00:06<00:00, 60.7MB/s]]\n",
      "Downloading data: 46.4MB [00:00, 73.3MB/s]/5 [00:08<00:14,  4.77s/it]\n",
      "Downloading data: 2.43MB [00:00, 64.3MB/s]                  3.90s/it]\n",
      "Downloading data: 2.11MB [00:00, 63.8MB/s]                  2.46s/it]\n",
      "Downloading data files: 100%|██████████| 5/5 [00:12<00:00,  2.45s/it]\n",
      "                                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset cnn_dailymail downloaded and prepared to /home/ubuntu/.cache/huggingface/datasets/cnn_dailymail/3.0.0/3.0.0/1b3c71476f6d152c31c1730e83ccb08bcf23e348233f4fcc11e182248e6bf7de. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00, 305.14it/s]\n",
      "Downloading (…)okenizer_config.json: 100%|██████████| 2.54k/2.54k [00:00<00:00, 830kB/s]\n",
      "Downloading (…)\"spiece.model\";: 100%|██████████| 792k/792k [00:00<00:00, 103MB/s]\n",
      "Downloading (…)/main/tokenizer.json: 100%|██████████| 2.42M/2.42M [00:00<00:00, 157MB/s]\n",
      "Downloading (…)cial_tokens_map.json: 100%|██████████| 2.20k/2.20k [00:00<00:00, 723kB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset size: 287113\n",
      "Test dataset size: 11490\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer\n",
    "import numpy as np \n",
    "\n",
    "\n",
    "# Load dataset from the hub\n",
    "dataset = load_dataset(dataset_id,name=dataset_config)\n",
    "# Load tokenizer of FLAN-t5-base\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "\n",
    "print(f\"Train dataset size: {len(dataset['train'])}\")\n",
    "print(f\"Test dataset size: {len(dataset['test'])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt lenght: 12\n",
      "Max input lenght: 500\n"
     ]
    }
   ],
   "source": [
    "prompt_lenght = len(tokenizer(prompt_template.format(input=\"\"))[\"input_ids\"])\n",
    "max_sample_length = tokenizer.model_max_length - prompt_lenght\n",
    "print(f\"Prompt lenght: {prompt_lenght}\")\n",
    "print(f\"Max input lenght: {max_sample_length}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.007428646087646484,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 299,
       "unit": "ba",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3dfc451289b94edd9675197fe6c194f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/299 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/ubuntu/.cache/huggingface/datasets/cnn_dailymail/3.0.0/3.0.0/1b3c71476f6d152c31c1730e83ccb08bcf23e348233f4fcc11e182248e6bf7de/cache-2243e7f93d3afe60.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max source length: 500\n",
      "Max target length: 129\n"
     ]
    }
   ],
   "source": [
    "from datasets import concatenate_datasets\n",
    "import numpy as np\n",
    "\n",
    "# The maximum total input sequence length after tokenization. \n",
    "# Sequences longer than this will be truncated, sequences shorter will be padded.\n",
    "tokenized_inputs = concatenate_datasets([dataset[\"train\"], dataset[\"test\"]]).map(lambda x: tokenizer(x[text_column], truncation=True), batched=True, remove_columns=[text_column, summary_column])\n",
    "max_source_length = max([len(x) for x in tokenized_inputs[\"input_ids\"]])\n",
    "max_source_length = min(max_source_length, max_sample_length)\n",
    "print(f\"Max source length: {max_source_length}\")\n",
    "\n",
    "# The maximum total sequence length for target text after tokenization. \n",
    "# Sequences longer than this will be truncated, sequences shorter will be padded.\"\n",
    "tokenized_targets = concatenate_datasets([dataset[\"train\"], dataset[\"test\"]]).map(lambda x: tokenizer(x[summary_column], truncation=True), batched=True, remove_columns=[text_column, summary_column])\n",
    "target_lenghts = [len(x) for x in tokenized_targets[\"input_ids\"]]\n",
    "# use 95th percentile as max target length\n",
    "max_target_length = int(np.percentile(target_lenghts, 95))\n",
    "print(f\"Max target length: {max_target_length}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_source_length=500\n",
    "max_target_length=129"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 288/288 [05:51<00:00,  1.22s/ba]\n",
      "100%|██████████| 14/14 [00:16<00:00,  1.15s/ba]\n",
      "100%|██████████| 12/12 [00:14<00:00,  1.17s/ba]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keys of tokenized dataset: ['input_ids', 'attention_mask', 'labels']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                   \r"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "def preprocess_function(sample,padding=\"max_length\"):\n",
    "    # created prompted input\n",
    "    inputs = [prompt_template.format(input=item) for item in sample[text_column]]\n",
    "\n",
    "    # tokenize inputs\n",
    "    model_inputs = tokenizer(inputs, max_length=max_source_length, padding=padding, truncation=True)\n",
    "\n",
    "    # Tokenize targets with the `text_target` keyword argument\n",
    "    labels = tokenizer(text_target=sample[summary_column], max_length=max_target_length, padding=padding, truncation=True)\n",
    "\n",
    "    # If we are padding here, replace all tokenizer.pad_token_id in the labels by -100 when we want to ignore\n",
    "    # padding in the loss.\n",
    "    if padding == \"max_length\":\n",
    "        labels[\"input_ids\"] = [\n",
    "            [(l if l != tokenizer.pad_token_id else -100) for l in label] for label in labels[\"input_ids\"]\n",
    "        ]\n",
    "\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n",
    "\n",
    "tokenized_dataset = dataset.map(preprocess_function, batched=True, remove_columns=list(dataset[\"train\"].features))\n",
    "print(f\"Keys of tokenized dataset: {list(tokenized_dataset['train'].features)}\")\n",
    "\n",
    "tokenized_dataset[\"train\"].save_to_disk(os.path.join(save_dataset_path,\"train\"))\n",
    "tokenized_dataset[\"test\"].save_to_disk(os.path.join(save_dataset_path,\"eval\"))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Launch Training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download(\"punkt\", quiet=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 scripts/run_seq2seq_deepspeed.py \\\n",
    "    --model_id $model_id \\\n",
    "    --dataset_path $save_dataset_path \\\n",
    "    --repository_id $repository_id \\\n",
    "    --epochs 3 \\\n",
    "    --per_device_train_batch_size 8 \\\n",
    "    --per_device_eval_batch_size 8 \\\n",
    "    --generation_max_length $max_target_length \\\n",
    "    --lr 5e-5 "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "46h on v100 with deespeed z3 offload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "deepspeed --num_gpus=4 scripts/run_seq2seq_deepspeed.py --model_id --dataset_path --epochs 3 --per_device_train_batch_size 8 --per_device_eval_batch_size 8 --generation_max_length --lr 5e-5 --deepspeed configs/ds_flan_t5_z3_config.json --repository_id\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "deepspeed --num_gpus=4 scripts/run_seq2seq_deepspeed.py --model_id google/flan-t5-base --dataset_path data --epochs 3 --per_device_train_batch_size 8 --per_device_eval_batch_size 8 --generation_max_length 129 --lr 1e-5 --deepspeed configs/ds_flan_t5_z3_config.json --repository_id flan-t5-base-cnn\n"
     ]
    }
   ],
   "source": [
    "!echo deepspeed --num_gpus=4 scripts/run_seq2seq_deepspeed.py \\\n",
    "    --model_id $model_id \\\n",
    "    --dataset_path $save_dataset_path \\\n",
    "    --epochs 3 \\\n",
    "    --per_device_train_batch_size 8 \\\n",
    "    --per_device_eval_batch_size 8 \\\n",
    "    --generation_max_length $max_target_length \\\n",
    "    --lr 1e-5 \\\n",
    "    --deepspeed configs/ds_flan_t5_z3_config.json \\\n",
    "    --repository_id $repository_id "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FLAN-T5 XL (3B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"google/flan-t5-xl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "deepspeed --num_gpus=4 scripts/run_seq2seq_deepspeed.py --model_id google/flan-t5-xl --dataset_path data --epochs 3 --per_device_train_batch_size 1 --per_device_eval_batch_size 1 --generation_max_length 129 --lr 1e-5 --deepspeed configs/ds_flan_t5_z3_config.json --repository_id flan-t5-base-cnn\n"
     ]
    }
   ],
   "source": [
    "!echo deepspeed --num_gpus=4 scripts/run_seq2seq_deepspeed.py \\\n",
    "    --model_id $model_id \\\n",
    "    --dataset_path $save_dataset_path \\\n",
    "    --epochs 3 \\\n",
    "    --per_device_train_batch_size 1 \\\n",
    "    --per_device_eval_batch_size 1 \\\n",
    "    --generation_max_length $max_target_length \\\n",
    "    --lr 1e-5 \\\n",
    "    --deepspeed configs/ds_flan_t5_z3_config.json \\\n",
    "    --repository_id $repository_id "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated memory needed for params, optim states and gradients for a:\n",
      "HW: Setup with 1 node, 4 GPUs per node.\n",
      "SW: Model with 2849M total params, 65M largest layer params.\n",
      "  per CPU  |  per GPU |   Options\n",
      "   71.66GB |   0.25GB | offload_param=cpu , offload_optimizer=cpu , zero_init=1\n",
      "   71.66GB |   0.25GB | offload_param=cpu , offload_optimizer=cpu , zero_init=0\n",
      "   63.70GB |   1.57GB | offload_param=none, offload_optimizer=cpu , zero_init=1\n",
      "   63.70GB |   1.57GB | offload_param=none, offload_optimizer=cpu , zero_init=0\n",
      "    1.47GB |  12.19GB | offload_param=none, offload_optimizer=none, zero_init=1\n",
      "   63.70GB |  12.19GB | offload_param=none, offload_optimizer=none, zero_init=0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/.local/lib/python3.8/site-packages/transformers/models/auto/modeling_auto.py:1238: FutureWarning: The class `AutoModelWithLMHead` is deprecated and will be removed in a future version. Please use `AutoModelForCausalLM` for causal language models, `AutoModelForMaskedLM` for masked language models and `AutoModelForSeq2SeqLM` for encoder-decoder models.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoConfig, AutoModelWithLMHead\n",
    "from accelerate import init_empty_weights\n",
    "from deepspeed.runtime.zero.stage3 import estimate_zero3_model_states_mem_needs_all_cold\n",
    "\n",
    "\n",
    "model_id=\"google/flan-t5-xl\"\n",
    "num_gpus=4\n",
    "\n",
    "# get parameters\n",
    "config = AutoConfig.from_pretrained(model_id)\n",
    "with init_empty_weights():\n",
    "    model = AutoModelWithLMHead.from_config(config)\n",
    "    model_parameter = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    largest_layer = max([p.numel() for p in model.parameters() if p.requires_grad])\n",
    "\n",
    "\n",
    "# calculate needed memory\n",
    "estimate_zero3_model_states_mem_needs_all_cold(model_parameter,largest_layer_params=largest_layer, num_gpus_per_node=num_gpus,num_nodes=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
