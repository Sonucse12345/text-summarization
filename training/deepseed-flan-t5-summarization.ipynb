{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!sudo apt install git-lfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install \"transformers==4.26.0\" \"datasets==2.9.0\" \"accelerate==0.16.0\" \"evaluate==0.4.0\"\n",
    "!pip install \"deepspeed==0.8.0\"\n",
    "!pip install rouge-score nltk py7zr tensorboard\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/dev/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Installed CUDA version 11.7 does not match the version torch was compiled with 11.2 but since the APIs are compatible, accepting this combination\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using /home/ubuntu/.cache/torch_extensions/py39_cu112 as PyTorch extensions root...\n",
      "Creating extension directory /home/ubuntu/.cache/torch_extensions/py39_cu112/cpu_adam...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /home/ubuntu/.cache/torch_extensions/py39_cu112/cpu_adam/build.ninja...\n",
      "Building extension module cpu_adam...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1/3] /opt/conda/envs/dev/bin/nvcc  -DTORCH_EXTENSION_NAME=cpu_adam -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1014\\\" -I/opt/conda/envs/dev/lib/python3.9/site-packages/deepspeed/ops/csrc/includes -I/opt/conda/envs/dev/include -isystem /opt/conda/envs/dev/lib/python3.9/site-packages/torch/include -isystem /opt/conda/envs/dev/lib/python3.9/site-packages/torch/include/torch/csrc/api/include -isystem /opt/conda/envs/dev/lib/python3.9/site-packages/torch/include/TH -isystem /opt/conda/envs/dev/lib/python3.9/site-packages/torch/include/THC -isystem /opt/conda/envs/dev/include -isystem /opt/conda/envs/dev/include/python3.9 -D_GLIBCXX_USE_CXX11_ABI=1 -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr -gencode=arch=compute_70,code=compute_70 -gencode=arch=compute_70,code=sm_70 --compiler-options '-fPIC' -O3 --use_fast_math -std=c++14 -U__CUDA_NO_HALF_OPERATORS__ -U__CUDA_NO_HALF_CONVERSIONS__ -U__CUDA_NO_HALF2_OPERATORS__ -gencode=arch=compute_70,code=sm_70 -gencode=arch=compute_70,code=compute_70 -c /opt/conda/envs/dev/lib/python3.9/site-packages/deepspeed/ops/csrc/common/custom_cuda_kernel.cu -o custom_cuda_kernel.cuda.o \n",
      "[2/3] c++ -MMD -MF cpu_adam.o.d -DTORCH_EXTENSION_NAME=cpu_adam -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1014\\\" -I/opt/conda/envs/dev/lib/python3.9/site-packages/deepspeed/ops/csrc/includes -I/opt/conda/envs/dev/include -isystem /opt/conda/envs/dev/lib/python3.9/site-packages/torch/include -isystem /opt/conda/envs/dev/lib/python3.9/site-packages/torch/include/torch/csrc/api/include -isystem /opt/conda/envs/dev/lib/python3.9/site-packages/torch/include/TH -isystem /opt/conda/envs/dev/lib/python3.9/site-packages/torch/include/THC -isystem /opt/conda/envs/dev/include -isystem /opt/conda/envs/dev/include/python3.9 -D_GLIBCXX_USE_CXX11_ABI=1 -fPIC -std=c++14 -O3 -std=c++14 -g -Wno-reorder -L/opt/conda/envs/dev/lib64 -lcudart -lcublas -g -march=native -fopenmp -D__AVX256__ -c /opt/conda/envs/dev/lib/python3.9/site-packages/deepspeed/ops/csrc/adam/cpu_adam.cpp -o cpu_adam.o \n",
      "[3/3] c++ cpu_adam.o custom_cuda_kernel.cuda.o -shared -lcurand -L/opt/conda/envs/dev/lib/python3.9/site-packages/torch/lib -lc10 -lc10_cuda -ltorch_cpu -ltorch_cuda -ltorch -ltorch_python -L/opt/conda/envs/dev/lib64 -lcudart -o cpu_adam.so\n",
      "\u001b[31mFAILED: \u001b[0mcpu_adam.so \n",
      "c++ cpu_adam.o custom_cuda_kernel.cuda.o -shared -lcurand -L/opt/conda/envs/dev/lib/python3.9/site-packages/torch/lib -lc10 -lc10_cuda -ltorch_cpu -ltorch_cuda -ltorch -ltorch_python -L/opt/conda/envs/dev/lib64 -lcudart -o cpu_adam.so\n",
      "/usr/bin/ld: cannot find -lcurand\n",
      "/usr/bin/ld: cannot find -lcudart\n",
      "collect2: error: ld returned 1 exit status\n",
      "ninja: build stopped: subcommand failed.\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Error building extension 'cpu_adam'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mCalledProcessError\u001b[0m                        Traceback (most recent call last)",
      "File \u001b[0;32m/opt/conda/envs/dev/lib/python3.9/site-packages/torch/utils/cpp_extension.py:1884\u001b[0m, in \u001b[0;36m_run_ninja_build\u001b[0;34m(build_directory, verbose, error_prefix)\u001b[0m\n\u001b[1;32m   1883\u001b[0m     stdout_fileno \u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m-> 1884\u001b[0m     subprocess\u001b[39m.\u001b[39;49mrun(\n\u001b[1;32m   1885\u001b[0m         command,\n\u001b[1;32m   1886\u001b[0m         stdout\u001b[39m=\u001b[39;49mstdout_fileno \u001b[39mif\u001b[39;49;00m verbose \u001b[39melse\u001b[39;49;00m subprocess\u001b[39m.\u001b[39;49mPIPE,\n\u001b[1;32m   1887\u001b[0m         stderr\u001b[39m=\u001b[39;49msubprocess\u001b[39m.\u001b[39;49mSTDOUT,\n\u001b[1;32m   1888\u001b[0m         cwd\u001b[39m=\u001b[39;49mbuild_directory,\n\u001b[1;32m   1889\u001b[0m         check\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m   1890\u001b[0m         env\u001b[39m=\u001b[39;49menv)\n\u001b[1;32m   1891\u001b[0m \u001b[39mexcept\u001b[39;00m subprocess\u001b[39m.\u001b[39mCalledProcessError \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m   1892\u001b[0m     \u001b[39m# Python 2 and 3 compatible way of getting the error object.\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/envs/dev/lib/python3.9/subprocess.py:528\u001b[0m, in \u001b[0;36mrun\u001b[0;34m(input, capture_output, timeout, check, *popenargs, **kwargs)\u001b[0m\n\u001b[1;32m    527\u001b[0m     \u001b[39mif\u001b[39;00m check \u001b[39mand\u001b[39;00m retcode:\n\u001b[0;32m--> 528\u001b[0m         \u001b[39mraise\u001b[39;00m CalledProcessError(retcode, process\u001b[39m.\u001b[39margs,\n\u001b[1;32m    529\u001b[0m                                  output\u001b[39m=\u001b[39mstdout, stderr\u001b[39m=\u001b[39mstderr)\n\u001b[1;32m    530\u001b[0m \u001b[39mreturn\u001b[39;00m CompletedProcess(process\u001b[39m.\u001b[39margs, retcode, stdout, stderr)\n",
      "\u001b[0;31mCalledProcessError\u001b[0m: Command '['ninja', '-v']' returned non-zero exit status 1.",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mdeepspeed\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m deepspeed\u001b[39m.\u001b[39;49mops\u001b[39m.\u001b[39;49mop_builder\u001b[39m.\u001b[39;49mCPUAdamBuilder()\u001b[39m.\u001b[39;49mload()\n",
      "File \u001b[0;32m/opt/conda/envs/dev/lib/python3.9/site-packages/deepspeed/ops/op_builder/builder.py:462\u001b[0m, in \u001b[0;36mOpBuilder.load\u001b[0;34m(self, verbose)\u001b[0m\n\u001b[1;32m    460\u001b[0m     \u001b[39mreturn\u001b[39;00m importlib\u001b[39m.\u001b[39mimport_module(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mabsolute_name())\n\u001b[1;32m    461\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 462\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mjit_load(verbose)\n",
      "File \u001b[0;32m/opt/conda/envs/dev/lib/python3.9/site-packages/deepspeed/ops/op_builder/builder.py:497\u001b[0m, in \u001b[0;36mOpBuilder.jit_load\u001b[0;34m(self, verbose)\u001b[0m\n\u001b[1;32m    494\u001b[0m     torch_arch_list \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39menviron\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mTORCH_CUDA_ARCH_LIST\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    495\u001b[0m     os\u001b[39m.\u001b[39menviron[\u001b[39m\"\u001b[39m\u001b[39mTORCH_CUDA_ARCH_LIST\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m--> 497\u001b[0m op_module \u001b[39m=\u001b[39m load(\n\u001b[1;32m    498\u001b[0m     name\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mname,\n\u001b[1;32m    499\u001b[0m     sources\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstrip_empty_entries(sources),\n\u001b[1;32m    500\u001b[0m     extra_include_paths\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstrip_empty_entries(extra_include_paths),\n\u001b[1;32m    501\u001b[0m     extra_cflags\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstrip_empty_entries(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcxx_args()),\n\u001b[1;32m    502\u001b[0m     extra_cuda_cflags\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstrip_empty_entries(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnvcc_args()),\n\u001b[1;32m    503\u001b[0m     extra_ldflags\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstrip_empty_entries(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mextra_ldflags()),\n\u001b[1;32m    504\u001b[0m     verbose\u001b[39m=\u001b[39;49mverbose)\n\u001b[1;32m    505\u001b[0m build_duration \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime() \u001b[39m-\u001b[39m start_build\n\u001b[1;32m    506\u001b[0m \u001b[39mif\u001b[39;00m verbose:\n",
      "File \u001b[0;32m/opt/conda/envs/dev/lib/python3.9/site-packages/torch/utils/cpp_extension.py:1268\u001b[0m, in \u001b[0;36mload\u001b[0;34m(name, sources, extra_cflags, extra_cuda_cflags, extra_ldflags, extra_include_paths, build_directory, verbose, with_cuda, is_python_module, is_standalone, keep_intermediates)\u001b[0m\n\u001b[1;32m   1176\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mload\u001b[39m(name,\n\u001b[1;32m   1177\u001b[0m          sources: Union[\u001b[39mstr\u001b[39m, List[\u001b[39mstr\u001b[39m]],\n\u001b[1;32m   1178\u001b[0m          extra_cflags\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1186\u001b[0m          is_standalone\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m   1187\u001b[0m          keep_intermediates\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m):\n\u001b[1;32m   1188\u001b[0m \u001b[39m    \u001b[39m\u001b[39mr\u001b[39m\u001b[39m'''\u001b[39;00m\n\u001b[1;32m   1189\u001b[0m \u001b[39m    Loads a PyTorch C++ extension just-in-time (JIT).\u001b[39;00m\n\u001b[1;32m   1190\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1266\u001b[0m \u001b[39m        ...     verbose=True)\u001b[39;00m\n\u001b[1;32m   1267\u001b[0m \u001b[39m    '''\u001b[39;00m\n\u001b[0;32m-> 1268\u001b[0m     \u001b[39mreturn\u001b[39;00m _jit_compile(\n\u001b[1;32m   1269\u001b[0m         name,\n\u001b[1;32m   1270\u001b[0m         [sources] \u001b[39mif\u001b[39;49;00m \u001b[39misinstance\u001b[39;49m(sources, \u001b[39mstr\u001b[39;49m) \u001b[39melse\u001b[39;49;00m sources,\n\u001b[1;32m   1271\u001b[0m         extra_cflags,\n\u001b[1;32m   1272\u001b[0m         extra_cuda_cflags,\n\u001b[1;32m   1273\u001b[0m         extra_ldflags,\n\u001b[1;32m   1274\u001b[0m         extra_include_paths,\n\u001b[1;32m   1275\u001b[0m         build_directory \u001b[39mor\u001b[39;49;00m _get_build_directory(name, verbose),\n\u001b[1;32m   1276\u001b[0m         verbose,\n\u001b[1;32m   1277\u001b[0m         with_cuda,\n\u001b[1;32m   1278\u001b[0m         is_python_module,\n\u001b[1;32m   1279\u001b[0m         is_standalone,\n\u001b[1;32m   1280\u001b[0m         keep_intermediates\u001b[39m=\u001b[39;49mkeep_intermediates)\n",
      "File \u001b[0;32m/opt/conda/envs/dev/lib/python3.9/site-packages/torch/utils/cpp_extension.py:1492\u001b[0m, in \u001b[0;36m_jit_compile\u001b[0;34m(name, sources, extra_cflags, extra_cuda_cflags, extra_ldflags, extra_include_paths, build_directory, verbose, with_cuda, is_python_module, is_standalone, keep_intermediates)\u001b[0m\n\u001b[1;32m   1488\u001b[0m                 hipified_sources\u001b[39m.\u001b[39madd(hipify_result[s_abs][\u001b[39m\"\u001b[39m\u001b[39mhipified_path\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39mif\u001b[39;00m s_abs \u001b[39min\u001b[39;00m hipify_result \u001b[39melse\u001b[39;00m s_abs)\n\u001b[1;32m   1490\u001b[0m             sources \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(hipified_sources)\n\u001b[0;32m-> 1492\u001b[0m         _write_ninja_file_and_build_library(\n\u001b[1;32m   1493\u001b[0m             name\u001b[39m=\u001b[39;49mname,\n\u001b[1;32m   1494\u001b[0m             sources\u001b[39m=\u001b[39;49msources,\n\u001b[1;32m   1495\u001b[0m             extra_cflags\u001b[39m=\u001b[39;49mextra_cflags \u001b[39mor\u001b[39;49;00m [],\n\u001b[1;32m   1496\u001b[0m             extra_cuda_cflags\u001b[39m=\u001b[39;49mextra_cuda_cflags \u001b[39mor\u001b[39;49;00m [],\n\u001b[1;32m   1497\u001b[0m             extra_ldflags\u001b[39m=\u001b[39;49mextra_ldflags \u001b[39mor\u001b[39;49;00m [],\n\u001b[1;32m   1498\u001b[0m             extra_include_paths\u001b[39m=\u001b[39;49mextra_include_paths \u001b[39mor\u001b[39;49;00m [],\n\u001b[1;32m   1499\u001b[0m             build_directory\u001b[39m=\u001b[39;49mbuild_directory,\n\u001b[1;32m   1500\u001b[0m             verbose\u001b[39m=\u001b[39;49mverbose,\n\u001b[1;32m   1501\u001b[0m             with_cuda\u001b[39m=\u001b[39;49mwith_cuda,\n\u001b[1;32m   1502\u001b[0m             is_standalone\u001b[39m=\u001b[39;49mis_standalone)\n\u001b[1;32m   1503\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m   1504\u001b[0m     baton\u001b[39m.\u001b[39mrelease()\n",
      "File \u001b[0;32m/opt/conda/envs/dev/lib/python3.9/site-packages/torch/utils/cpp_extension.py:1607\u001b[0m, in \u001b[0;36m_write_ninja_file_and_build_library\u001b[0;34m(name, sources, extra_cflags, extra_cuda_cflags, extra_ldflags, extra_include_paths, build_directory, verbose, with_cuda, is_standalone)\u001b[0m\n\u001b[1;32m   1605\u001b[0m \u001b[39mif\u001b[39;00m verbose:\n\u001b[1;32m   1606\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mBuilding extension module \u001b[39m\u001b[39m{\u001b[39;00mname\u001b[39m}\u001b[39;00m\u001b[39m...\u001b[39m\u001b[39m'\u001b[39m, file\u001b[39m=\u001b[39msys\u001b[39m.\u001b[39mstderr)\n\u001b[0;32m-> 1607\u001b[0m _run_ninja_build(\n\u001b[1;32m   1608\u001b[0m     build_directory,\n\u001b[1;32m   1609\u001b[0m     verbose,\n\u001b[1;32m   1610\u001b[0m     error_prefix\u001b[39m=\u001b[39;49m\u001b[39mf\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mError building extension \u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39m{\u001b[39;49;00mname\u001b[39m}\u001b[39;49;00m\u001b[39m'\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n",
      "File \u001b[0;32m/opt/conda/envs/dev/lib/python3.9/site-packages/torch/utils/cpp_extension.py:1900\u001b[0m, in \u001b[0;36m_run_ninja_build\u001b[0;34m(build_directory, verbose, error_prefix)\u001b[0m\n\u001b[1;32m   1898\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(error, \u001b[39m'\u001b[39m\u001b[39moutput\u001b[39m\u001b[39m'\u001b[39m) \u001b[39mand\u001b[39;00m error\u001b[39m.\u001b[39moutput:  \u001b[39m# type: ignore[union-attr]\u001b[39;00m\n\u001b[1;32m   1899\u001b[0m     message \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m: \u001b[39m\u001b[39m{\u001b[39;00merror\u001b[39m.\u001b[39moutput\u001b[39m.\u001b[39mdecode(\u001b[39m*\u001b[39mSUBPROCESS_DECODE_ARGS)\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m  \u001b[39m# type: ignore[union-attr]\u001b[39;00m\n\u001b[0;32m-> 1900\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(message) \u001b[39mfrom\u001b[39;00m \u001b[39me\u001b[39;00m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Error building extension 'cpu_adam'"
     ]
    }
   ],
   "source": [
    "import deepspeed\n",
    "deepspeed.ops.op_builder.CPUAdamBuilder().load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading channels: done\n",
      "# Name                       Version           Build  Channel             \n",
      "pytorch                       1.13.1 py3.8_cuda11.7_cudnn8.5.0_0  pytorch             \n",
      "\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "conda search -c pytorch pytorch=1.13.1=py3.8_cuda11.7*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'11.2'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "torch.version.cuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "DeepSpeed C++/CUDA extension op report\n",
      "--------------------------------------------------\n",
      "NOTE: Ops not installed will be just-in-time (JIT) compiled at\n",
      "      runtime if needed. Op compatibility means that your system\n",
      "      meet the required dependencies to JIT install the op.\n",
      "--------------------------------------------------\n",
      "JIT compiled ops requires ninja\n",
      "ninja .................. \u001b[92m[OKAY]\u001b[0m\n",
      "--------------------------------------------------\n",
      "op name ................ installed .. compatible\n",
      "--------------------------------------------------\n",
      "\u001b[93m [WARNING] \u001b[0m async_io requires the dev libaio .so object and headers but these were not found.\n",
      "\u001b[93m [WARNING] \u001b[0m async_io: please install the libaio-dev package with apt\n",
      "\u001b[93m [WARNING] \u001b[0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.\n",
      "async_io ............... \u001b[93m[NO]\u001b[0m ....... \u001b[93m[NO]\u001b[0m\n",
      "cpu_adagrad ............ \u001b[93m[NO]\u001b[0m ....... \u001b[92m[OKAY]\u001b[0m\n",
      "cpu_adam ............... \u001b[93m[NO]\u001b[0m ....... \u001b[92m[OKAY]\u001b[0m\n",
      "fused_adam ............. \u001b[93m[NO]\u001b[0m ....... \u001b[92m[OKAY]\u001b[0m\n",
      "fused_lamb ............. \u001b[93m[NO]\u001b[0m ....... \u001b[92m[OKAY]\u001b[0m\n",
      "quantizer .............. \u001b[93m[NO]\u001b[0m ....... \u001b[92m[OKAY]\u001b[0m\n",
      "random_ltd ............. \u001b[93m[NO]\u001b[0m ....... \u001b[92m[OKAY]\u001b[0m\n",
      "\u001b[93m [WARNING] \u001b[0m please install triton==1.0.0 if you want to use sparse attention\n",
      "sparse_attn ............ \u001b[93m[NO]\u001b[0m ....... \u001b[93m[NO]\u001b[0m\n",
      "spatial_inference ...... \u001b[93m[NO]\u001b[0m ....... \u001b[92m[OKAY]\u001b[0m\n",
      "transformer ............ \u001b[93m[NO]\u001b[0m ....... \u001b[92m[OKAY]\u001b[0m\n",
      "stochastic_transformer . \u001b[93m[NO]\u001b[0m ....... \u001b[92m[OKAY]\u001b[0m\n",
      "transformer_inference .. \u001b[93m[NO]\u001b[0m ....... \u001b[92m[OKAY]\u001b[0m\n",
      "utils .................. \u001b[93m[NO]\u001b[0m ....... \u001b[92m[OKAY]\u001b[0m\n",
      "--------------------------------------------------\n",
      "DeepSpeed general environment info:\n",
      "torch install path ............... ['/opt/conda/envs/dev/lib/python3.9/site-packages/torch']\n",
      "torch version .................... 1.13.1.post200\n",
      "deepspeed install path ........... ['/opt/conda/envs/dev/lib/python3.9/site-packages/deepspeed']\n",
      "deepspeed info ................... 0.8.0, unknown, unknown\n",
      "torch cuda version ............... 11.2\n",
      "torch hip version ................ None\n",
      "nvcc version ..................... 11.7\n",
      "deepspeed wheel compiled w. ...... torch 1.13, cuda 11.2\n"
     ]
    }
   ],
   "source": [
    "!ds_report"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# experiment config\n",
    "model_id = \"google/flan-t5-base\"\n",
    "repository_id = \"flan-t5-base-cnn\"\n",
    "\n",
    "# Dataset \n",
    "dataset_id = \"cnn_dailymail\"\n",
    "dataset_config = \"3.0.0\"\n",
    "save_dataset_path = \"data\"\n",
    "text_column = \"article\"\n",
    "summary_column = \"highlights\"\n",
    "prompt_start = \"Summarize the following news article:\\n\"\n",
    "generation_start = \"\\nSummary:\\n\"\n",
    "prompt_template = f\"{prompt_start}{{input}}{generation_start}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset cnn_dailymail (/home/ubuntu/.cache/huggingface/datasets/cnn_dailymail/3.0.0/3.0.0/1b3c71476f6d152c31c1730e83ccb08bcf23e348233f4fcc11e182248e6bf7de)\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.007609367370605469,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 3,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ea6a863f22a74d949adcd135ea5de43a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset size: 287113\n",
      "Test dataset size: 11490\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer\n",
    "import numpy as np \n",
    "\n",
    "\n",
    "# Load dataset from the hub\n",
    "dataset = load_dataset(dataset_id,name=dataset_config)\n",
    "# Load tokenizer of FLAN-t5-base\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "\n",
    "print(f\"Train dataset size: {len(dataset['train'])}\")\n",
    "print(f\"Test dataset size: {len(dataset['test'])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt lenght: 12\n",
      "Max input lenght: 500\n"
     ]
    }
   ],
   "source": [
    "prompt_lenght = len(tokenizer(prompt_template.format(input=\"\"))[\"input_ids\"])\n",
    "max_sample_length = tokenizer.model_max_length - prompt_lenght\n",
    "print(f\"Prompt lenght: {prompt_lenght}\")\n",
    "print(f\"Max input lenght: {max_sample_length}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.007428646087646484,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 299,
       "unit": "ba",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3dfc451289b94edd9675197fe6c194f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/299 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/ubuntu/.cache/huggingface/datasets/cnn_dailymail/3.0.0/3.0.0/1b3c71476f6d152c31c1730e83ccb08bcf23e348233f4fcc11e182248e6bf7de/cache-2243e7f93d3afe60.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max source length: 500\n",
      "Max target length: 129\n"
     ]
    }
   ],
   "source": [
    "from datasets import concatenate_datasets\n",
    "import numpy as np\n",
    "\n",
    "# The maximum total input sequence length after tokenization. \n",
    "# Sequences longer than this will be truncated, sequences shorter will be padded.\n",
    "tokenized_inputs = concatenate_datasets([dataset[\"train\"], dataset[\"test\"]]).map(lambda x: tokenizer(x[text_column], truncation=True), batched=True, remove_columns=[text_column, summary_column])\n",
    "max_source_length = max([len(x) for x in tokenized_inputs[\"input_ids\"]])\n",
    "max_source_length = min(max_source_length, max_sample_length)\n",
    "print(f\"Max source length: {max_source_length}\")\n",
    "\n",
    "# The maximum total sequence length for target text after tokenization. \n",
    "# Sequences longer than this will be truncated, sequences shorter will be padded.\"\n",
    "tokenized_targets = concatenate_datasets([dataset[\"train\"], dataset[\"test\"]]).map(lambda x: tokenizer(x[summary_column], truncation=True), batched=True, remove_columns=[text_column, summary_column])\n",
    "target_lenghts = [len(x) for x in tokenized_targets[\"input_ids\"]]\n",
    "# use 95th percentile as max target length\n",
    "max_target_length = int(np.percentile(target_lenghts, 95))\n",
    "print(f\"Max target length: {max_target_length}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_target_length=129"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.009089469909667969,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 288,
       "unit": "ba",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa04a927a741405db2d0c07892802f70",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/288 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.007318735122680664,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 14,
       "unit": "ba",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ec5411288bc2447c94de1e6c5c68bc2c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/14 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.007582187652587891,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 12,
       "unit": "ba",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f80591fe8d9e4276971c6d18ecf290fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/12 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keys of tokenized dataset: ['input_ids', 'attention_mask', 'labels']\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.007813692092895508,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Saving the dataset (0/3 shards)",
       "rate": null,
       "total": 287113,
       "unit": " examples",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c205381b6c6841ccb70a606e96458ee6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/3 shards):   0%|          | 0/287113 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.007582187652587891,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Saving the dataset (0/1 shards)",
       "rate": null,
       "total": 11490,
       "unit": " examples",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "94b5d556a4304108b245991762f64c88",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/11490 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "def preprocess_function(sample,padding=\"max_length\"):\n",
    "    # created prompted input\n",
    "    inputs = [prompt_template.format(input=item) for item in sample[text_column]]\n",
    "\n",
    "    # tokenize inputs\n",
    "    model_inputs = tokenizer(inputs, max_length=max_source_length, padding=padding, truncation=True)\n",
    "\n",
    "    # Tokenize targets with the `text_target` keyword argument\n",
    "    labels = tokenizer(text_target=sample[summary_column], max_length=max_target_length, padding=padding, truncation=True)\n",
    "\n",
    "    # If we are padding here, replace all tokenizer.pad_token_id in the labels by -100 when we want to ignore\n",
    "    # padding in the loss.\n",
    "    if padding == \"max_length\":\n",
    "        labels[\"input_ids\"] = [\n",
    "            [(l if l != tokenizer.pad_token_id else -100) for l in label] for label in labels[\"input_ids\"]\n",
    "        ]\n",
    "\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n",
    "\n",
    "tokenized_dataset = dataset.map(preprocess_function, batched=True, remove_columns=list(dataset[\"train\"].features))\n",
    "print(f\"Keys of tokenized dataset: {list(tokenized_dataset['train'].features)}\")\n",
    "\n",
    "tokenized_dataset[\"train\"].save_to_disk(os.path.join(save_dataset_path,\"train\"))\n",
    "tokenized_dataset[\"test\"].save_to_disk(os.path.join(save_dataset_path,\"eval\"))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Launch Training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download(\"punkt\", quiet=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0%|                                   | 131/107670 [00:59<13:39:54,  2.19it/s]^C\n"
     ]
    }
   ],
   "source": [
    "!python scripts/run_seq2seq_deepspeed.py \\\n",
    "    --model_id $model_id \\\n",
    "    --dataset_path $save_dataset_path \\\n",
    "    --repository_id $repository_id \\\n",
    "    --epochs 3 \\\n",
    "    --per_device_train_batch_size 8 \\\n",
    "    --per_device_eval_batch_size 8 \\\n",
    "    --generation_max_length $max_target_length \\\n",
    "    --lr 5e-5 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-02-03 15:35:15,007] [WARNING] [runner.py:186:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.\n",
      "[2023-02-03 15:35:15,022] [INFO] [runner.py:548:main] cmd = /opt/conda/envs/pytorch/bin/python3.9 -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMF19 --master_addr=127.0.0.1 --master_port=29500 --enable_each_rank_log=None scripts/run_seq2seq_deepspeed.py --model_id google/flan-t5-base --dataset_path data --epochs 3 --per_device_train_batch_size 16 --per_device_eval_batch_size 16 --generation_max_length 129 --lr 5e-5 --deepspeed configs/ds_flan_t5_z3_config.json --repository_id flan-t5-base-cnn \\\n",
      "[2023-02-03 15:35:17,522] [INFO] [launch.py:142:main] WORLD INFO DICT: {'localhost': [0]}\n",
      "[2023-02-03 15:35:17,523] [INFO] [launch.py:148:main] nnodes=1, num_local_procs=1, node_rank=0\n",
      "[2023-02-03 15:35:17,523] [INFO] [launch.py:161:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0]})\n",
      "[2023-02-03 15:35:17,523] [INFO] [launch.py:162:main] dist_world_size=1\n",
      "[2023-02-03 15:35:17,523] [INFO] [launch.py:164:main] Setting CUDA_VISIBLE_DEVICES=0\n",
      "[2023-02-03 15:35:26,361] [INFO] [comm.py:657:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl\n",
      "/home/ubuntu/deep-learning-pytorch-huggingface/training/flan-t5-base-cnn is already a clone of https://huggingface.co/philschmid/flan-t5-base-cnn. Make sure you pull the latest changes with `repo.git_pull()`.\n",
      "[2023-02-03 15:35:30,755] [INFO] [logging.py:68:log_dist] [Rank 0] DeepSpeed info: version=0.8.0, git-hash=unknown, git-branch=unknown\n",
      "[2023-02-03 15:35:31,014] [INFO] [logging.py:68:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False\n",
      "Using /home/ubuntu/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /home/ubuntu/.cache/torch_extensions/py39_cu117/cpu_adam/build.ninja...\n",
      "Building extension module cpu_adam...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "[1/3] c++ -MMD -MF cpu_adam.o.d -DTORCH_EXTENSION_NAME=cpu_adam -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -I/opt/conda/envs/pytorch/lib/python3.9/site-packages/deepspeed/ops/csrc/includes -I/opt/conda/envs/pytorch/include -isystem /opt/conda/envs/pytorch/lib/python3.9/site-packages/torch/include -isystem /opt/conda/envs/pytorch/lib/python3.9/site-packages/torch/include/torch/csrc/api/include -isystem /opt/conda/envs/pytorch/lib/python3.9/site-packages/torch/include/TH -isystem /opt/conda/envs/pytorch/lib/python3.9/site-packages/torch/include/THC -isystem /opt/conda/envs/pytorch/include -isystem /opt/conda/envs/pytorch/include/python3.9 -D_GLIBCXX_USE_CXX11_ABI=0 -fPIC -std=c++14 -O3 -std=c++14 -g -Wno-reorder -L/opt/conda/envs/pytorch/lib64 -lcudart -lcublas -g -march=native -fopenmp -D__AVX256__ -c /opt/conda/envs/pytorch/lib/python3.9/site-packages/deepspeed/ops/csrc/adam/cpu_adam.cpp -o cpu_adam.o \n",
      "\u001b[31mFAILED: \u001b[0mcpu_adam.o \n",
      "c++ -MMD -MF cpu_adam.o.d -DTORCH_EXTENSION_NAME=cpu_adam -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -I/opt/conda/envs/pytorch/lib/python3.9/site-packages/deepspeed/ops/csrc/includes -I/opt/conda/envs/pytorch/include -isystem /opt/conda/envs/pytorch/lib/python3.9/site-packages/torch/include -isystem /opt/conda/envs/pytorch/lib/python3.9/site-packages/torch/include/torch/csrc/api/include -isystem /opt/conda/envs/pytorch/lib/python3.9/site-packages/torch/include/TH -isystem /opt/conda/envs/pytorch/lib/python3.9/site-packages/torch/include/THC -isystem /opt/conda/envs/pytorch/include -isystem /opt/conda/envs/pytorch/include/python3.9 -D_GLIBCXX_USE_CXX11_ABI=0 -fPIC -std=c++14 -O3 -std=c++14 -g -Wno-reorder -L/opt/conda/envs/pytorch/lib64 -lcudart -lcublas -g -march=native -fopenmp -D__AVX256__ -c /opt/conda/envs/pytorch/lib/python3.9/site-packages/deepspeed/ops/csrc/adam/cpu_adam.cpp -o cpu_adam.o \n",
      "In file included from /opt/conda/envs/pytorch/lib/python3.9/site-packages/deepspeed/ops/csrc/includes/context.h:11,\n",
      "                 from /opt/conda/envs/pytorch/lib/python3.9/site-packages/deepspeed/ops/csrc/includes/custom_cuda_layers.h:15,\n",
      "                 from /opt/conda/envs/pytorch/lib/python3.9/site-packages/deepspeed/ops/csrc/includes/cpu_adam.h:11,\n",
      "                 from /opt/conda/envs/pytorch/lib/python3.9/site-packages/deepspeed/ops/csrc/adam/cpu_adam.cpp:1:\n",
      "/opt/conda/envs/pytorch/lib/python3.9/site-packages/deepspeed/ops/csrc/includes/gemm_test.h:6:10: fatal error: cuda_profiler_api.h: No such file or directory\n",
      "    6 | #include <cuda_profiler_api.h>\n",
      "      |          ^~~~~~~~~~~~~~~~~~~~~\n",
      "compilation terminated.\n",
      "[2/3] /opt/conda/envs/pytorch/bin/nvcc  -DTORCH_EXTENSION_NAME=cpu_adam -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -I/opt/conda/envs/pytorch/lib/python3.9/site-packages/deepspeed/ops/csrc/includes -I/opt/conda/envs/pytorch/include -isystem /opt/conda/envs/pytorch/lib/python3.9/site-packages/torch/include -isystem /opt/conda/envs/pytorch/lib/python3.9/site-packages/torch/include/torch/csrc/api/include -isystem /opt/conda/envs/pytorch/lib/python3.9/site-packages/torch/include/TH -isystem /opt/conda/envs/pytorch/lib/python3.9/site-packages/torch/include/THC -isystem /opt/conda/envs/pytorch/include -isystem /opt/conda/envs/pytorch/include/python3.9 -D_GLIBCXX_USE_CXX11_ABI=0 -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr -gencode=arch=compute_70,code=compute_70 -gencode=arch=compute_70,code=sm_70 --compiler-options '-fPIC' -O3 --use_fast_math -std=c++14 -U__CUDA_NO_HALF_OPERATORS__ -U__CUDA_NO_HALF_CONVERSIONS__ -U__CUDA_NO_HALF2_OPERATORS__ -gencode=arch=compute_70,code=sm_70 -gencode=arch=compute_70,code=compute_70 -c /opt/conda/envs/pytorch/lib/python3.9/site-packages/deepspeed/ops/csrc/common/custom_cuda_kernel.cu -o custom_cuda_kernel.cuda.o \n",
      "\u001b[31mFAILED: \u001b[0mcustom_cuda_kernel.cuda.o \n",
      "/opt/conda/envs/pytorch/bin/nvcc  -DTORCH_EXTENSION_NAME=cpu_adam -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -I/opt/conda/envs/pytorch/lib/python3.9/site-packages/deepspeed/ops/csrc/includes -I/opt/conda/envs/pytorch/include -isystem /opt/conda/envs/pytorch/lib/python3.9/site-packages/torch/include -isystem /opt/conda/envs/pytorch/lib/python3.9/site-packages/torch/include/torch/csrc/api/include -isystem /opt/conda/envs/pytorch/lib/python3.9/site-packages/torch/include/TH -isystem /opt/conda/envs/pytorch/lib/python3.9/site-packages/torch/include/THC -isystem /opt/conda/envs/pytorch/include -isystem /opt/conda/envs/pytorch/include/python3.9 -D_GLIBCXX_USE_CXX11_ABI=0 -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr -gencode=arch=compute_70,code=compute_70 -gencode=arch=compute_70,code=sm_70 --compiler-options '-fPIC' -O3 --use_fast_math -std=c++14 -U__CUDA_NO_HALF_OPERATORS__ -U__CUDA_NO_HALF_CONVERSIONS__ -U__CUDA_NO_HALF2_OPERATORS__ -gencode=arch=compute_70,code=sm_70 -gencode=arch=compute_70,code=compute_70 -c /opt/conda/envs/pytorch/lib/python3.9/site-packages/deepspeed/ops/csrc/common/custom_cuda_kernel.cu -o custom_cuda_kernel.cuda.o \n",
      "In file included from /opt/conda/envs/pytorch/lib/python3.9/site-packages/deepspeed/ops/csrc/includes/context.h:11,\n",
      "                 from /opt/conda/envs/pytorch/lib/python3.9/site-packages/deepspeed/ops/csrc/includes/custom_cuda_layers.h:15,\n",
      "                 from /opt/conda/envs/pytorch/lib/python3.9/site-packages/deepspeed/ops/csrc/common/custom_cuda_kernel.cu:1:\n",
      "/opt/conda/envs/pytorch/lib/python3.9/site-packages/deepspeed/ops/csrc/includes/gemm_test.h:6:10: fatal error: cuda_profiler_api.h: No such file or directory\n",
      "    6 | #include <cuda_profiler_api.h>\n",
      "      |          ^~~~~~~~~~~~~~~~~~~~~\n",
      "compilation terminated.\n",
      "ninja: build stopped: subcommand failed.\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/envs/pytorch/lib/python3.9/site-packages/torch/utils/cpp_extension.py\", line 1900, in _run_ninja_build\n",
      "    subprocess.run(\n",
      "  File \"/opt/conda/envs/pytorch/lib/python3.9/subprocess.py\", line 528, in run\n",
      "    raise CalledProcessError(retcode, process.args,\n",
      "subprocess.CalledProcessError: Command '['ninja', '-v']' returned non-zero exit status 1.\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/deep-learning-pytorch-huggingface/training/scripts/run_seq2seq_deepspeed.py\", line 176, in <module>\n",
      "    main()\n",
      "  File \"/home/ubuntu/deep-learning-pytorch-huggingface/training/scripts/run_seq2seq_deepspeed.py\", line 173, in main\n",
      "    training_function(args)  \n",
      "  File \"/home/ubuntu/deep-learning-pytorch-huggingface/training/scripts/run_seq2seq_deepspeed.py\", line 161, in training_function\n",
      "    trainer.train()\n",
      "  File \"/opt/conda/envs/pytorch/lib/python3.9/site-packages/transformers/trainer.py\", line 1543, in train\n",
      "    return inner_training_loop(\n",
      "  File \"/opt/conda/envs/pytorch/lib/python3.9/site-packages/transformers/trainer.py\", line 1612, in _inner_training_loop\n",
      "    deepspeed_engine, optimizer, lr_scheduler = deepspeed_init(\n",
      "  File \"/opt/conda/envs/pytorch/lib/python3.9/site-packages/transformers/deepspeed.py\", line 344, in deepspeed_init\n",
      "    deepspeed_engine, optimizer, _, lr_scheduler = deepspeed.initialize(**kwargs)\n",
      "  File \"/opt/conda/envs/pytorch/lib/python3.9/site-packages/deepspeed/__init__.py\", line 125, in initialize\n",
      "    engine = DeepSpeedEngine(args=args,\n",
      "  File \"/opt/conda/envs/pytorch/lib/python3.9/site-packages/deepspeed/runtime/engine.py\", line 340, in __init__\n",
      "    self._configure_optimizer(optimizer, model_parameters)\n",
      "  File \"/opt/conda/envs/pytorch/lib/python3.9/site-packages/deepspeed/runtime/engine.py\", line 1276, in _configure_optimizer\n",
      "    basic_optimizer = self._configure_basic_optimizer(model_parameters)\n",
      "  File \"/opt/conda/envs/pytorch/lib/python3.9/site-packages/deepspeed/runtime/engine.py\", line 1347, in _configure_basic_optimizer\n",
      "    optimizer = DeepSpeedCPUAdam(model_parameters,\n",
      "  File \"/opt/conda/envs/pytorch/lib/python3.9/site-packages/deepspeed/ops/adam/cpu_adam.py\", line 95, in __init__\n",
      "    self.ds_opt_adam = get_accelerator().create_op_builder(CPUAdamBuilder).load()\n",
      "  File \"/opt/conda/envs/pytorch/lib/python3.9/site-packages/deepspeed/ops/op_builder/builder.py\", line 462, in load\n",
      "    return self.jit_load(verbose)\n",
      "  File \"/opt/conda/envs/pytorch/lib/python3.9/site-packages/deepspeed/ops/op_builder/builder.py\", line 497, in jit_load\n",
      "    op_module = load(\n",
      "  File \"/opt/conda/envs/pytorch/lib/python3.9/site-packages/torch/utils/cpp_extension.py\", line 1284, in load\n",
      "    return _jit_compile(\n",
      "  File \"/opt/conda/envs/pytorch/lib/python3.9/site-packages/torch/utils/cpp_extension.py\", line 1508, in _jit_compile\n",
      "    _write_ninja_file_and_build_library(\n",
      "  File \"/opt/conda/envs/pytorch/lib/python3.9/site-packages/torch/utils/cpp_extension.py\", line 1623, in _write_ninja_file_and_build_library\n",
      "    _run_ninja_build(\n",
      "  File \"/opt/conda/envs/pytorch/lib/python3.9/site-packages/torch/utils/cpp_extension.py\", line 1916, in _run_ninja_build\n",
      "    raise RuntimeError(message) from e\n",
      "RuntimeError: Error building extension 'cpu_adam'\n",
      "Exception ignored in: <function DeepSpeedCPUAdam.__del__ at 0x7f64e18ace50>\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/envs/pytorch/lib/python3.9/site-packages/deepspeed/ops/adam/cpu_adam.py\", line 109, in __del__\n",
      "AttributeError: 'DeepSpeedCPUAdam' object has no attribute 'ds_opt_adam'\n",
      "[2023-02-03 15:35:37,551] [INFO] [launch.py:318:sigkill_handler] Killing subprocess 6667\n",
      "[2023-02-03 15:35:37,552] [ERROR] [launch.py:324:sigkill_handler] ['/opt/conda/envs/pytorch/bin/python3.9', '-u', 'scripts/run_seq2seq_deepspeed.py', '--local_rank=0', '--model_id', 'google/flan-t5-base', '--dataset_path', 'data', '--epochs', '3', '--per_device_train_batch_size', '16', '--per_device_eval_batch_size', '16', '--generation_max_length', '129', '--lr', '5e-5', '--deepspeed', 'configs/ds_flan_t5_z3_config.json', '--repository_id', 'flan-t5-base-cnn', '\\\\'] exits with return code = 1\n"
     ]
    }
   ],
   "source": [
    "!deepspeed --num_gpus=1 scripts/run_seq2seq_deepspeed.py \\\n",
    "    --model_id $model_id \\\n",
    "    --dataset_path $save_dataset_path \\\n",
    "    --epochs 3 \\\n",
    "    --per_device_train_batch_size 16 \\\n",
    "    --per_device_eval_batch_size 16 \\\n",
    "    --generation_max_length $max_target_length \\\n",
    "    --lr 5e-5 \\\n",
    "    --deepspeed configs/ds_flan_t5_z3_config.json \\\n",
    "    --repository_id $repository_id \\"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "2d58e898dde0263bc564c6968b04150abacfd33eed9b19aaa8e45c040360e146"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
